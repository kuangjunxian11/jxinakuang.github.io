---

layout: post
title: llm评测
category: 技术
tags: MachineLearning
keywords: llm Evaluation

---

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$']], // 支持 $和$$ 作为行内公式分隔符
      displayMath: [['$$', '$$']], // 块级公式分隔符
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script async src="/public/js/mathjax/es5/tex-mml-chtml.js"></script>

* TOC
{:toc}

## 简介(未完成)

[揭秘大模型评测：如何用“说明书”式方法实现业务场景下的精准评估](https://mp.weixin.qq.com/s/uW_CCjKmSBKrD76Du2GLLQ)

大模型评测的目标是通过设计合理的测试任务和数据集来对大模型的能力进行全面、量化的评估。
1. 性能测试通过压测实现。
2. 基础模型的Benchmark(基准测试)
3. 业务效果方面的评测。在基础模型发布时，模型厂商提供的测试报告无法覆盖用户实际业务场景。用户需要通过针对自己业务场景设计的评测来评估大模型的实际表现。
    1. 针对模型本身，
    2. 面向整个模型应用进行评测，覆盖RAG、MCP、工作流等构成的统一整体进行端到端的测试。

## 挑战

大模型评测相关工具平台随着各大模型平台厂商的持续投入以及开源社区的火爆，相关功能也在持续完善，目前已经不是主要的困难点。目前的主要困难更多聚焦在如何结合自己的业务场景开展大模型评测。
1. 评测维度：评测维度如何设计才能更好的衡量大模型效果，并推动大模型优化？
2. 评测集：如何设计评测集才能更好地仿真实际的线上场景。如何平衡不同场景的比例失衡的问题，确保不同场景的覆盖？
3. 标注：标注人员质量参差不齐，不同人对标准的理解不一致。同一个人不同时间标注，也会导致结果不同，最终导致标注准确性稳定性差。除了标注效果外，人工标注非常耗时且需要投入额外的人工成本，导致无法开展大规模评测。
4. 业务变化：随着技术方案和业务场景的变化，大模型本身也在持续迭代演进。不同大模型特点不同，评测标准和评测集的构成也各不相同。

## 评测流程

![](/public/upload/machine/llm_evaluation_process.png)

1. 评测维度设计：举个例子，比如在“AI试衣”场景里，我们预先定义了“清晰度与分辨率”、“色彩准确性”、“人体与服饰的自然融合”、“姿势与角度匹配”、“光影与背景一致性”共5个评测维度。但是实际消费者使用时反馈部分场景的bad case在评测时给出好的评价。了解分析后发现消费者表示“我的体型穿这种衣服根本不是这个效果”。那基于消费者的这个反馈，我们就需要增加评测维度“人体比例与体型适配”。
2. 模型效果量化。在完成分维度的设计后有一个最终的问题是，如何将各个维度的效果汇总成模型的整体效果的度量，从而评估模型是变好还是变坏，是否达到了上线等后续动作的标准。不同维度的优先级是不同的，安全类、客户投诉类一般都高于其他的业务指标。通过权重调整各个维度的重要性，最终通过加权后的得分来量化模型的最终效果，是一种相对比较简单的实现方案。对于总分，通过加权$ f(x) = \sum_{1 \leq i \leq n} a_i \bar{x}_i $ 计算总体效果。其中 $ n $ 表示场景数量，$ \bar{x}_i $ 表示某个场景的平均分，$ a_i $ 表示该场景样本的权重，可以设置为该场景样本的占比，也可以都是 1，或者业务人员根据业务需求设定。 
