---

layout: post
title: 推理LLM梳理
category: 技术
tags: MachineLearning
keywords: llm agent

---

* TOC
{:toc}

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

## 简介（未完成）

[作为开发者，我如何提高任务型大模型应用的响应性能](https://mp.weixin.qq.com/s/_4s8HiRASW59V9S0YMRRww) 减少输出token、选择合适尺寸的模型以及采用流式输出。

张俊林：目前可以提高模型效果的Scaling方法，按照性价比由高到低排序的话: Test time Scaling Law> RL Scaling Law>预训练阶段Scaling Law(数据不够了，只能推大模型尺寸)。如果哪天RL Scaling Law和Test Time Scaling Law到了天花板，又没有找到新的性价比更合算的Scaling law，也不是说模型效果就提不上去了，大家仍然可以回归预训练阶段的Scaling Law，没有新数据也没关系，推大模型尺寸规模就可以，效果仍然会上升。然后RL阶段Scaling 的天花板随之升高，然后可以再去Scale RL和Test Time，就进一步得到智商更高的大模型。如果这成立，那意味着AGI的解决方案已经完整了？

## 扯扯闲篇


当大家在网上探索o1是如何训练时，肯定会看到以下几个热点词：
1. Test/Inference-Time scaling law，通过增加推理阶段的算力提升模型的推理能力
2. Post Training，通过后训练提升模型的推理能力
3. PRM/ORM：基于过程/结果的奖励模型
4. CoT：思维链
5. 强化学习、self-play（自我博弈）与MCTS（使用蒙特卡洛搜索树寻找最佳答案）

### 什么是Test/Inference-time Scaling Law

[OpenAI o1模型的本质优势是什么？ - 猛猿的回答 - 知乎](https://www.zhihu.com/question/667055619/answer/3864887300)设想一下，当我们手里有一个基础模型（我们称其为generator），但是这个模型的逻辑推理能力（比如解数学题的能力）较差时，我们该怎么改进它？再说的具体点，不考虑数据集相关的成本，假设我手头的gpu算力（FLOPs）是有限的，我该怎么利用它，能让我的模型最终能推理出更好的结果？一个比较直接的想法是：把算力花在它的pretain阶段，给模型注入更多数理逻辑的预训练知识。例如用更好、更多的代码数学等数据，或者扩展模型的参数规模。这个做法启发自大家都很熟悉的scaling law（更具体地说是pretrain-time scaling law）。但是，当我们研读openai o1的技术报告时，我们会发现，它把这个算力更多地用在了2个地方：

1. 用在了rlhf的训练上（post training）
2. 用在了模型的推理阶段上（Test/Inferece）
正如pretrain scaling law受到模型参数和训练数据的影响一样，**Test/Inferece scaling law也必然受某些因素影响，而这些因素是什么，又是怎么影响的？**不过等等，此时你肯定想问：
1. 一般来说，一个模型的效果是由它的训练阶段决定的，所以如果这里说通过pretrain或者post training来提升模型的推理能力，我都能理解。但是inference阶段是怎么提升模型的推理能力的？你说的把算力用在inference阶段到底是什么意思？
2. post training和inferece是两种独立的提升模型推理能力的方法吗？它们可以结合在一起使用吗？

把算力用在inference阶段，也就是说，在不变动pretrain阶段的情况下，只通过推理等层面的优化，来提升模型最后的生成效果。这里又分成两种情况。
1. 优化推理输入：prompt。这个方法大家应该非常熟悉了。例如，原来你的模型吃一个问题，直接吐给你回答。但是现在为了让模型能更好模拟人类的思考方式，你希望【模型在步步思考后再给出回答，也就是模型的生成结果里包含思考步骤+答案】，那么你可以选择在prompt中给模型相应的例子，或者在多轮对话中引导模型think step by step，来实现这个目标。你的prompt给的越细节，你的多轮引导给的越多，模型或许就能产出更好的结果。比如DSPy 的APE。难点是，要么**一个问题给一个这样的prompt，要么所有问题共用一个这样的prompt**。
2. 优化推理输出：revise output distribution。可是，优化推理输入的方法还是不够直接。难道对于每一个问题，我需要精心设计prompt，或者手动诱导模型think step by step才行。**所以能不能让模型吃下一个问题后，自动化地去做CoT的过程呢？**也就是说，现在我们希望模型在吃下一个问题后，能自主产生以下输出：
`attempt1 -> attempt2 -> attempt3 -> ...-> attempti -> answer`，其中，每个attempt包含“多个中间步骤steps + 最终结果”，也就是它在模拟人类的思考过程：先做一次attempt，然后发现问题，在此基础上在做别的attempt，直到找到最终答案。那么我要怎么让模型做到这点呢，一个直观的方法就是，如果我有：
`problem -> attempt1 -> ... -> attempti -> answer` 这种带标签的数据，那我不就能直接训练了？训练的方法也有很多，例如：
1. 我直接做sft，把最正确的attempt放在输入序列最后，当作label进行训练即可
2. 我用类似rlhf的方法，先有一个奖励模型，它能对每一个思考步骤做评估，然后利用这个评估结果，指引模型步步搜索，每一步都找到最佳的思考步骤，最后不就能找到答案了？
PS：Inference-time体现在，当用户输入一个问题之后，o1要花费更长的时间进行「思考」，其实也就是在生成最终答案之前，先生成了很多reasoning tokens。所以就得训练llm不要直接给答案（心直口快），得养成step by step（甚至带上反思）的”本能“

这两种解法，仅从训练方法上来说，都可以算成是post-training，也就是我们通过把算力花在post-training上来提升模型的逻辑推理能力。可是，本文的标题不是【把算力花在inference上】吗？inference在哪里呢？我们再重新端详这2种解法：
1. 假设我们使用解法1或者解法2 post training好了模型，现在我们拿它做推理。模型吃一个问题，产出一系列中间结果和答案，但是你能保证，这些中间结果和答案一定是最好的吗？
2. 所以此时，一方面，我们可以考虑优化推理阶段，即使用一个能够评估中间步骤的verifier，在推理时指引模型搜索出最佳答案。例如，我们对一个问题采样多个attempts链，从中找最好的。或者在单个attempts中找到最好的attempt，诸如此类。
3. 而另一方面，我们可以考虑在post-training阶段，使用这个verifier来指导模型自动化生产高质量的数据（这是个inference步骤），基于这些数据我们再做对齐。如果这个流程做得好，我们甚至可以直接信任post-training后模型的结果
所以，【优化推理输出】这一部分，你可以把算力全部花在post-training上，也可以花在post-training+inference上，从o1的技术报告上看，它应该选择了后者，同时post-training选择了某种基于强化学习的方法（其实o1在pretrain阶段应该也有变动，具体的分析我们在后文中会通过实验数据给出猜想）。至此，我们就把问题1和问题2都回答清楚了。

![](/public/upload/machine/o1_generate_verifier.jpg)

一个能按照格式，产出中间思考步骤的模型（generator），但中间思考步骤质量得不到保证。一个能对中间思考步骤进行评估的奖励模型PRM（verifier）。而现在我们想做的事情是：如何在不对generator继续做任何训练的情况下，使用verfier，来引导generator搜索出最佳的“steps + answer”？
1. 使用PRM指导搜索过程。
2. 直接改变模型的输出分布

## test-time Compute

[可视化角度具象化理解DeepSeek-R1类推理大模型的习得进程](https://mp.weixin.qq.com/s/ytKTGTgU2T7jSNrBghX1cA)

train-time compute ==> test-time Compute; Scaling Laws ==> Inference/test-time scaling

test-time Compute不是不断增加预训练预算，而是允许模式在推理过程中“思考更长时间” 。

![](/public/upload/machine/test_time_compute.jpg)

test-time Compute 可以是很多不同的东西，包括思路链、修改答案、回溯、采样等等。这些可以大致分为两类：
1. 一个是针对验证者进行搜索Search against Verifiers（抽样生成并选择最佳答案），以输出为中心。
2. 一个是修改提议分布Modifying Proposal Distribution（训练“思考”过程），以输入为中心。

![](/public/upload/machine/verifier_vs_modify_proposal_distribution.jpg)

但这几种都是需要打分奖励的，有两种类型的验证器，一个是结果奖励模型（ORM），一个是流程奖励模型（PRM），ORM只判断结果，并不关心底层过程：相比之下，PRM还会判断导致结果的过程（“推理”）。

![](/public/upload/machine/orm_vs_prm.png)

### 各种 Search against Verifiers

大概思路：首先创建多个推理过程和答案的样本，验证者（奖励模型）对生成的输出进行评分，使用验证器的一个主要优点是不需要重新训练或微调用于回答问题的LLM。

![](/public/upload/machine/search_against_verifiers.jpg)

可以细分为以下几种子类别：
1. 多数表决，也就是投票Majority Voting，最直接的方法其实不是使用奖励模型或验证器，而是进行多数投票。让模型生成多个答案，生成次数最多的答案将作为最终答案。这种方法也称为自洽，以强调生成多个答案和推理步骤的必要性。
    ![](/public/upload/machine/majority_voting.jpg)
2. 最佳N样本Best-of-N samples。LLM（通常称为提议者）使用高温或变化的温度生成多个答案。每个答案都会经过输出奖励模型 (ORM)，并根据答案的质量进行评分。得分最高的答案将被选中。除了判断答案之外，推理过程还可以通过过程奖励模型(PRM) 来判断，该模型会判断每个推理步骤的质量。会选择总权重最高的候选答案。还可以通过 RM 对每个答案候选者进行加权，并选出总权重最高的答案。这称为加权 Best-of-N 样本。
3. 使用过程奖励模型进行集束搜索Beam search with process reward models。生成答案和中间步骤的过程可以通过定向搜索进一步扩展。使用定向搜索，可以抽取多个推理步骤，每个步骤都由PRM进行判断，整个过程都会跟踪排名前3位的“beams”（得分最高的路径），快速停止那些没有结果的“推理”路径。PS: 用prm替代llm输出的logits softmax
    ![](/public/upload/machine/beam_search_with_prm.jpg)
4. 蒙特卡洛树搜索Monte Carlo Tree Search。包括四个步骤：选择（根据预先确定的公式选择给定的叶子）->扩展（创建更多节点）->推出（随机创建新节点，直到到达终点）->反向传播（根据输出更新父节点分数），这些步骤的主要目标是不断扩展最佳推理步骤，同时探索其他路径。因此，这是探索与利用之间的平衡。节点评分和选择方式的示例如下：
    ![](/public/upload/machine/mcts_selection_score.jpg)
    当选择一个新的推理步骤进行探索时，它不一定是迄今为止表现最佳的路径。使用这种类型的公式，首先选择一个节点（推理步骤），然后通过生成新的推理步骤来扩展它。和以前一样，这可以通过合理高且变化的温度值来完成：
    ![](/public/upload/machine/mcts_rm.png)
    选择其中一个扩展推理步骤，并进行多次，直到得出多个答案。这些举措可以根据推理步骤（PRM）、奖励（ORM）或两者的结合来判断。

### 修改提议分布Modifying Proposal Distribution

这种方式的模型不再使用验证器（以输出为中心）搜索正确的推理步骤，而是经过训练以创建改进的推理步骤（以输入为中心），换句话说，对完成/想法/标记进行采样的分布被修改了。假设有一个问题和一个分布，我们可以从中抽取 token。一个常见的策略是获取得分最高的 token：

![](/public/upload/machine/choose_not_highest_score.jpg)

但是，请注意上图中的某些标记被标记为红色。这些标记更有可能引发推理过程：

![](/public/upload/machine/choose_more_reasoning_token.jpg)

虽然选择贪婪token不一定是错误的，但选择一个引发推理过程的令牌往往会得到更好的答案。**当修改提议分布（标记概率分布）时，本质上是让模型对分布进行重新排序，以便更频繁地选择“推理”标记**：

![](/public/upload/machine/choose_more_reasoning_token2.jpg)

修改提案分布的方法有很多种，但一般可以分为两类，通过提示工程更新提示或者训练模型以关注推理标记/过程。
1. 提示Prompting。通过提示工程，尝试通过更新提示来改进输出。为了通过提示改变提议分布，可以向模型提供示例（上下文学习），这个过程可以进一步简化，只需说“让我们一步一步思考”( “Let’s think step-by-step”)。同样，这也改变了提案的分布，使得LLM 倾向于在回答之前分解整个过程。然而，模型本身并没有学会遵循这个过程。此外，这是一个静态的线性过程，会阻碍自我完善。如果模型以错误的推理过程开始，它往往会保留它，而不是修改它。
    ![](/public/upload/machine/choose_reasoning_by_cot.jpg)
2. STaR。可以通过训练让模型学会“推理”，这样模型在生成这些推理步骤时就会得到奖励。这通常需要大量推理数据和强化学习来奖励某些行为。一种备受争议的技术被称为STaR，即自学推理机。STaR是一种利用LLM生成自身推理数据作为微调模型的输入的方法。它会生成推理步骤和答案。如果答案正确，则将推理和答案添加到三元组训练数据集`<question,reasoning,answer>`，此数据用于对模型进行监督微调。如果模型给出了错误的答案，那么我们会提供一个“提示”（正确答案），并要求模型推理为什么这个答案是正确的，最后的推理步骤是添加相同的三元组训练数据，用于对模型进行监督微调。
    ![](/public/upload/machine/star.jpg)
    
## 如何增强大语言模型的推理能力/推理LLM

[理解推理 LLM：构建和改进推理模型的方法与策略](https://mp.weixin.qq.com/s/qCLs7EbiAKcG8tafOrU4iQ)在本文中，我将"推理"定义为回答需要复杂的、多步骤生成且包含中间步骤的问题的过程。例如，"法国的首都是什么?"这样的事实性问答并不涉及推理。相比之下，"如果一列火车以 60 英里/小时的速度行驶 3 小时，它会行驶多远?"这样的问题需要一些简单的推理。推理模型通常会在回答中包含中间步骤，以部分展现思维过程。大多数现代 LLM 都具备基础推理能力，能够回答诸如"如果火车以每小时 60 英里的速度行驶 3 小时，它能走多远？"这类问题。因此，如今当我们提及推理模型时，通常指的是那些擅长处理更复杂推理任务（如解谜题、破解智力题和完成数学证明）的 LLM。

![](/public/upload/machine/llm_vs_reasoning_llm.jpg)

什么时候我们需要推理模型？ 推理模型被设计用来擅长解决复杂任务，比如解谜题、高等数学问题和具有挑战性的编程任务。然而，对于更简单的任务，如总结、翻译或基于知识的问答，并不需要推理模型。事实上，在所有任务中都使用推理模型可能会效率低下且成本高昂。例如，推理模型通常使用成本更高，输出更冗长，有时由于"过度思考"而更容易出错。

构建和改进推理模型的 4 种主要方法，以DeepSeek R1训练过程来说
1. 推理时扩展。指的是在推理过程中增加计算资源以提高输出质量。
    1. 一个直接方法是巧妙的提示工程。一个经典的例子是思维链(CoT)提示，即在输入提示中包含"一步步思考"这样的短语。这鼓励模型生成中间推理步骤，而不是直接跳到最终答案，这在更复杂的问题上通常(但不总是)能带来更准确的结果。
    2. 另一种推理时扩展的方法是采用投票和搜索策略。一个简单的例子是多数投票法，即让 LLM 生成多个答案，并通过多数表决选择正确答案。类似地，我们可以使用束搜索（beam search）和其他搜索算法来生成更优的响应。
2. 纯强化学习（RL）/DeepSeek-R1-Zero。DeepSeek R1 论文中最让人印象深刻的发现之一是，推理能力是从纯强化学习（RL）中自然产生的行为。在奖励方面，他们没有使用基于人类偏好训练的奖励模型，而是采用了两种类型的奖励：准确度奖励和格式奖励。PS：deepseek 论文用了一个词 reasoning-oriented RL 
    1. 准确度奖励使用 LeetCode 编译器验证编程答案，并使用确定性系统评估数学答案。
    2. 格式奖励依靠 LLM 判断器来确保回答遵循预期格式，比如将推理步骤放在 `<think>` 标签内。
3. 监督式微调和强化学习（SFT + RL）
    1. 使用 DeepSeek-R1-Zero 生成了SFT 数据通过指令微调训练了模型，接着又进行了一轮强化学习（RL）阶段。这个 RL 阶段保留了 DeepSeek-R1-Zero 的 RL 过程中使用的相同准确度和格式奖励。不过，他们增加了一个一致性奖励，以防止语言混合现象，即模型在回答中混用多种语言的情况。
    2. RL 阶段之后是另一轮 SFT 数据收集。在这个阶段，他们使用最新的模型检查点生成了 60 万个思维链（CoT）SFT 样本，同时使用 DeepSeek-V3 基础模型创建了额外的 20 万个基于知识的 SFT 样本。这些 60 万 + 20 万个 SFT 样本随后被用于另一轮 RL。在这个阶段，他们再次对数学和编程问题使用基于规则的方法进行准确度奖励，而对其他类型的问题则使用人类偏好标签。最终的模型 DeepSeek-R1 相比 DeepSeek-R1-Zero 有显著的性能提升
4. 纯监督式微调（SFT）和蒸馏。这里的蒸馏指的是在由更大的 LLM 生成的 SFT 数据集上对较小的 LLM（如 Llama 8B 和 70B 以及 Qwen 2.5 模型（0.5B 到 32B））进行指令微调。结果表明对于较小的模型来说，蒸馏远比纯 RL 更有效。PS：对小模型来说，方法4比方法2有效。

## 其它
### 融合LLM和RL来生成Hidden COT

先聊一个问题，在知识掌握层面上，sft 后的模型为什么不如 pretrain 模型效果好？或者说，为什么 sft 后的模型在知识掌握上会有幻觉？
1. sft 在做什么？在找一条捷径，让 pretrain 模型可以直接说出答案，而不是续写一堆 token 后再总结出答案。
2. 为什么走捷径会产生幻觉呢，我举个例子：中国的首都是哪里呢？
    1. pretrain 模型：这个问题问得好…… 中国最早的首都是…… 中国现在的首都是……
    2. sft 模型：北京。
3. pretrain 模型有没有这个知识？一定有。pretrain 模型需要多少个 token 才能说出这个知识？不知道。运气好的时候续写一百个 token 就提到了北京，运气不好的时候续写一千个、一万个都有可能。那么问题来了，sft 模型走捷径而抛弃的这一千个 token，到底有没有信息量呢？到底是不是推导出中国的首都是北京的关键 cot 过程呢？大概率是有的，一旦学会了这种走捷径的方式，并且把这种捷径泛化到其他知识上，模型的幻觉也就产生了。这里，一定不能总是用人思考的方式来揣摩机器思考的方式，我们认为“中国的首都是北京”是天经地义的几个 token 就学会的知识，模型可能是从《北京的发展史》这一本几万 token 的书籍中才学到的这个知识。然后我就猜测：把 prompt 喂给 pretrain 模型，先续写 1W 个 token，再总结这 1W 个 token 得到 response，训练和推理的时候都不省略这 1W 个 token，这种方式估计大概率不会让模型产生幻觉，因为模型根本没学会走捷径。这就有点o1 的味道了。
4. 所谓捷径就是极其稀少的语言或者文本相关性分布，和原有分布不一致。所以模型无所适从，就是幻觉。

### RL从未走远

[LLM的范式转移：RL带来新的 Scaling Law](https://mp.weixin.qq.com/s/JPfgF6UtgIYwWXwNQHOoqQ)2018 年，Lex Fridman 邀请 Ilya 来 MIT 客座讲一节课，Ilya 选择的主题是 RL 和 self-play，因为他认为这是通往 AGI 的路上最关键的方法之一。Ilya 在讲座中用一句话概括了强化学习：让 AI 用随机路径去尝试一个新的任务，如果效果超出预期，就更新神经网络的权重让 AI 记得多使用成功的实践，然后开始下一次尝试。这个概括中可以看到强化学习和其他 AI 范式的重要区别，经典三大范式（监督学习、非监督学习、强化学习）中只有强化学习的假设是让 AI 进行自主探索、连续决策，这个学习方式最接近人类的学习方式，也符合我们想象中的 AI agent 应该具备的自主行动能力。强化学习的核心在于"探索"（Explore）和"利用"（Exploit）之间的权衡。LLM 在"利用"现有知识上做到了现阶段的极致，而在"探索"新知识方面还有很大潜力，RL 的引入就是为了让 LLM 能通过探索进一步提升推理能力。在实现 RL 的过程中，有两个核心组件。他们之间一直在反复交互，agent 在环境中执行 action，并且根据环境的变化评估 reward：

1. Environment：AI 探索完成任务的环境，当 Alphago 下围棋时，环境就是 19x19 的棋盘。环境会发生变化，AI 会从环境变化中收到 reward value 判断过去的那一系列探索是否有明显的收益，例如距离下围棋胜利是否更接近了。
2. Agent：agent 会根据对环境的观测和感知来输出一个动作，目标是得到更高的 reward。agent 这个概念最早就是来自强化学习。
PS: Environment ==> reward ==> agent ==> action ==> environment ==> reward ==> agent ==> action ==> ...
如果把这里的 agent 主体换成 LLM，那么会在探索的过程中做很多 LLM inference。因此这里 RL 在 LLM 中应用的思路本质是用 inference time 换 training time，来解决模型 scale up 暂时边际收益递减的现状。LLM 直接生成是可以类比系统 1 的慢思考。而 RL 就为 LLM 带来了系统 2 慢思考。

### RLHF下一步 O1

思维链让LLM初步学会了像人类System 2一样的思考复杂问题的模式，但这还远远不够。一个明显的gap是：LLM在训练过程中并没有足够多的包含思维链的训练数据，但却在推理阶段被要求以思维链的方式思考问题。如果在训练阶段使用包含更多含思维链的数据，那么模型在需要推理的任务上的表现会更好。由此，一条提升LLM性能的道路清晰可见：构造包含思维链的数据，将其用于LLM的训练阶段以提升LLM的推理能力，使其降低幻觉。

[万字长文解析OpenAI o1 Self-Play RL技术路线](https://mp.weixin.qq.com/s/wJURZu61LoxiXHtKUT3jIA)OpenAI o1 可以通过 Self-Play 的方式提升模型 Reasoning 的能力，o1 是怎么实现这样的能力呢，纯粹从推理态来看是 inference time thinking 做到的，就是在回答用户问题之前，模型会陷入一个思考的过程。逐步思考，提出假设，并且反思，以实现 Reasoning 能力。这里面的 thinking 流程是模型和其他大模型最大的不同，在这中间经历了相当长时间的思考阶段。思考的内容，目前在 ChatGPT 的客户端中可以做了隐藏（防止被蒸馏）。虽然不清楚背后实现的具体逻辑，但是从目前已有的接口来看，o1 至少已经能够实现：提出假设，验证思路，反思过程这三种主要的逻辑推理能力。

大语言模型的主要学习策略从 RLHF 的巨大成功之后，也出现过摇摆。以 next token prediction 作为代表的 Behavior Clone 思路主要的手段是预训练和 SFT 为主的，主要强调从海量知识中自监督学习加上专家数据的示教。但是这一条路径遇到了很大的困难，我们如今已经几乎耗尽了几乎所有互联网上所有的语料，但是极强的智能也没有出现。同时 SFT 作为 Behavior Clone 的上限是比较低的，大多数情况下需要堆叠大量高质量语料，成本几乎成为了垂直领域难以负担的问题。更大的问题在于 SFT 几乎无法囊括负例的示教，对于 trial-n-error 的自我博弈智能来说，只能利用其中比例极低的正例。所以祖师爷 John Schulman 的 PPO 加上 RLHF 力挽狂澜，把 GPT-3 拉出黑暗，直接进化到 InstructGPT，用人类反馈进行建模引爆了整个领域。但是我们**现在又到了一个十字路口，大模型看起来好像是一个死记硬背的书呆子，推理能力迟迟没有见到突飞猛进的变化**，大模型 Self-Play 能否通过部分领域示教数据，模型通过自我博弈持续提升策略？这里面需要有两个先决条件：Generator 和 Verifier 都要足够强。语言和游戏在这个方面是截然相反的，游戏中的行为生成是困难的而价值评判是简单的：对于路边看棋大爷下好一步棋很难，但是判断这一步下的好不好他还是可以的。语言模型生成行为是容易的，但是判断生成的好坏是困难的，1B 的模型都可以滔滔不绝证明哥德巴赫猜想，但是判断每一步是否正确却非常困难。

这一切正在悄然改变，Reward 数据正在越变越多，作为 Verifier 的 Reward Model（RM）也在变得越来越强。我们看到了越来越多的证据，新的的 scaling 趋势呈现在了生成式 RM 上2。这种 Reward Model 相比于传统的方法来说，对于大语言模型的判别已经不是一锤子买卖了。它更像是人类标注员的思路，对问题和答案会和传统生成式模型一样也能够进行 CoT。他会对于一个问题和答案，首先按照生成式模型的方法给出自然语言的判断，然后再给出 RL 所需要的标量数值，彻底摆脱了判别式 RM 中 BT 假设的枷锁。所以随着 Reward Model 思考的深入，其准确度也会不断上涨。同时更重要的是，verifer 和 generator 之间也可以通过信息密度更高的自然语言的方式进行互动。相当于 RM 监督 policy 的时候，不仅告诉了每条答案的评分还详细给出了错误的原因。这种以自然语言作为交互模式的对抗 + 合作的模式可以随着计算资源的增长获得明显的增长（推演的更多，反思的更细）。其中的对抗是，大语言模型要经历生成更好的回答让 RM 无法挑出问题，而 RM 也要自己增长能力以发现大语言模型的更多漏洞。合作则在于，最终两者的博弈并不是零和的，两者的同步增长会使得我们的大语言模型拥有真正的长思考能力，并有机会往全领域泛化。

那么第二个问题是：Verifier 判别出来的正例和负例是不是同时能够利用起来，答案是比较正面的。而且强化学习中，引入负例可以更有效地提升大语言模型的推理强度。数据利用效率更是达到了仅使用正例的八倍，这个结论是非常好理解的，对于推理来说一个巨大的采用空间内，做错的可能性在起初要大大高于能够做对的概率。如果无法充分利用负例的数据价值，学习效率就会大打折扣。

从推理时的较为确定的 Self-Play 方式出发，我们可以反向推演一下 o1 的可能技术路线（声明一下这些都是推演）。假设 Generator 和 Verifier 是两个相互配合的模型，部署的时候使用两个模型组成的系统，那么就可以使用 actor-critic 的方式加 TD-error 来更新 generator model 和 verifier model。PS： 细节可以看原文，有点get 到 self-play 啥意思了。

LLM scaling up 的边际收益开始递减，用 RL self-play + MCTS 提升 LLM 推理能力成为下一个技术范式。在新范式下，LLM 领域的 scaling law 会发生变化：计算量变大仍会带来模型智能的提升，但会从模型参数量变大，转移到 inference-time compute 增加，也就是模型进行更多 RL 探索。要让 RL 算法能够在连续推理任务上做到最好，理解 self-play + MCTS 的思路是最重要的。放到 LLM 语境下，self-play 是让 LLM 同时扮演一个或多个 agent model 去做推理任务，并由另一个 LLM 作为 reward model 来给出打分评价，一定次数后更新 LLM 权重让其多记住做得好的推理方式。Self-play RL 是要在好的策略上持续探索，怎么定义“好”就尤其重要。因此， Reward model（奖励模型） 是 RL 中最关键的模块之一，有两个关键的卡点是需要解决的，那就是 reward model 的泛化性和连续性。Self-play RL 在棋牌、电子游戏、数学竞赛上之所以有效，是因为这些领域都有明确的胜负标准，可以作为 reward model 的基础。有了 LLM 的 in-context learning，我们相信代码、数学是可以通过 LLM + self-play RL 来持续进步的。根据 The information 报道，strawberry 目前能力最强的领域就在 math 和 code 上，Sonnet 3.5 在代码的提升也是很好的佐证。这两个领域具有准确、快迭代的评判标准，使得模型能够获得明确的反馈：我们可以把 code script 放进 Python Interpreter/ compiler（如果不成功，报错信息也能帮助 AI 自己去发现和理解错误在哪里），把 math proof 放进 Lean（Lean 是一种编程语言，通过计算机验证数据定理，广泛用在 AI 形式化数学证明中帮助 AI 理解数学题），就能自动验证其准确性。

**reward model 对其他领域的泛化性**并不明确。物理、医药有明确的标准答案，但需要很长的实验验证周期。法律、金融的问题往往没有通用解法，很难用通用的 reward model 实现。文字创意领域的 reward 很多时候不符合马尔可夫模型，也就是其 reward 常常会有跳变。一本好的小说、剧本，会讲究反转，试想 LLM next-token prediction 到一个反转之前其 reward 函数还很低，一个精彩的反转让 reward 函数突然大幅提升，self-play RL 很难捕捉这个突然的变化。**因此这里孕育着新范式下的第二个创业机会：垂直领域的 reward model**。而要让 reward function 能捕捉到更多的信号，在垂直领域之外泛化，最重要的方向就是怎么用好 LLM 作为 reward model，并同时输出数字和文字评估。


LLM as a PRM （process reward model）：通往泛化的重要路线。在传统 RL 中，我们按照最终结果评分，其评分模型称为 ORM（outcome reward model）；而通过专门训练 LLM 成为 process verifier ，新的评分模型叫做 PRM，往往是使用娇小 LLM fine-tune 得到。PRM （Process reward model） 是奖励好的推理步骤，而不仅仅是正确的结果。这更接近人类的学习和推理方式。而且在 process supervision 过程中，reward 的形式也不止限于数值，文字评价也可以作为指导模型继续行动的 reward。PS：如何训练一个好的PRM [逻辑推理与决策规划（二）：过程监督与结果监督](https://zhuanlan.zhihu.com/p/17569409591)

[左脚踩右脚真能上天？谷歌提出自我纠正RL提升LLM](https://mp.weixin.qq.com/s/OnPPizOC1Ae63WjhNdLYNA)光靠模型自己给自己打分，较难获得新的信息增量提升效果，要么需要多个模型，要么依赖于更高级的模型或其他形式的监督（如RAG提供外部输入）。但**最强的那个模型要怎么继续进步呢**？光靠模型自己反思真的不行吗？谷歌DeepMind团队提出多轮在线强化学习方法 SCoRe，完全使用自生成的数据进行训练，不需要任何外部反馈模型/信号，就能实现内在自我修正策略以“即时”修正自己的错误，提升了模型的推理能力。我们理性的看，这种“自我反思”要有效果，有一个重要前提，是当前LLMs已经压缩了和问题答案相关的信息/知识，但还无法很灵活的使用，需要更高效的策略帮它提取&运用起来。


## 0123材料

[谈谈对DeepSeek-R1的一些理解](https://mp.weixin.qq.com/s/pSR8RyvvQPUEuHBtUFh8Og) 在OpenAI o1刚放出来时，它有限的技术报告里，有2个内容格外抓人眼球：Inference/test-time scaling 和 RL
1. Inference/test-time scaling：这一块的主要作用是**为RL过程自动化地制造高质量数据集**。包括用于format模型产生思考过程的long cot数据集，以及带preference labels的数据集。我把这一块的系统抽象为PRM + some search methods的形式。例如讨论度很高的MCTS，本质上也可理解为 fixed PRM + some search methods。PS：test-time就是Inference-time，只是历史的命名习惯问题。
2. 这部分应该就是openAI自己惯有的一套RL流程。
在这样的训练框架下，最终推理时是否要再次引入inference-time scaling模块，就是一个可选项了。只要RL过程做得充分好，那么直接用训完的policy模型就可以，完全不需要再做优化。那么，我为什么当时会认为 inference-time scaling 和 RL 应该是2个独立的过程呢？因为在我的认知里，我认为如果没有显式的引导，模型是不具备产生long cot（乃至带反思的cot）的能力的（在模型训练初期，这个能力是指formatting模型，让它知道要产出这种格式的回答；在训练过程中再来慢慢提升这种回答的质量）这个显示引导就是指诸如sft这样的过程。PS：先引导思考，再评价思考对不对

而直到前几天，又是蹭着热点读到了dpsk-r1的这篇技术报告，我这下才发现：原来单纯的RL就可以激发模型产出带有long cot（甚至是反思）的回复的能力！这里单纯的RL是指：我并没有显式提供一些真正的long cot数据让模型去背去学，我只是在sys_msg里告诉模型先思考，再回答。**接着通过RL一轮又一轮的训练，模型产出的responses越来越长，且在某个时刻出现了自我评估和反思的行为**。这个实验探索就是dpsk-r1-zero在做的事情。如果RL有这种能力，那么inference time scaling 和 RL 就可以不是2个独立的过程，而是在RL的过程里自发出现了inference time scaling的现象，而如果它们不再独立，那么类o1的训练架构也许就比我们想得要简单很多。

低成本快速增强大模型逻辑推理能力的方法  
1. 首先，找到大量<问题，答案>数据，包括STEM、代码、数学、逻辑等方面题目集合；  
2. 第二，对问题进行改写，从问题侧来扩充<问题，答案>数据的数量；  
3. 第三，引入开源的类o1模型，比如Deepseek发布的各种R1开源模型，  
4. 第四，使用R1模型制作推理轨迹数据，并标注出问题的难易程度：可以通过对问题使用R1模型多次Rollout生成推理步骤轨迹，比如一个问题生成8个推理轨迹，根据最终正确答案是否正确进行过滤，过滤掉最终答案错误的例子；形成<问题，推理轨迹COT，答案>数据集合。  
5. 第五，（此步骤可选非必需，但可能比较重要）找到一个好的PRM模型，比如阿里开源的PRM模型，对某个推理轨迹COT整体质量进行评估，比如回答某个问题的推理轨迹由10个推理步骤构成，根据每个推理步骤PRM得分的平均分，得出整个推理轨迹的得分，得分低者意味着轨迹中包含错误推理步骤比较多，说明整体质量低，把整体质量低的<问题，推理轨迹COT，答案>数据过滤掉，只用剩下的高质量推理轨迹数据。这一步等于提升推理步骤整体正确率，等价于提升训练数据质量。 
6. 第六，使用剩下的高质量正例对基座模型进行SFT，数据使用顺序采取课程学习思路，由简单题目到难题，或者逐步增加难题比例，由此得到最强逻辑推理模型 
7. 第七，如果你想让自己的技术方案看着更有技术含量一些，可以引入部分负例（最终答案错误的推理轨迹COT数据），结合正例做个DPO，我感觉是这步骤可选非必需。

这本质上是种数据蒸馏方法，好处是成本极低、实现起来速度快，能很快制作当前最强逻辑推理模型，如果都这么做，那谁更强取决于三个因素：谁有更多的题目，尤其是难题；类o1模型给出的推理轨迹质量，质量高者胜出；PRM模型的准确性，更准确者胜。但是这个方法的缺点是缺乏自我进化机制，上面三个因素基本共同决定了能达到的效果天花板，缺乏通过模型不断迭代增强能力的机制。

[对 deepseek R1 和 K1.5 技术报告的一些思考](https://zhuanlan.zhihu.com/p/19838650037) 都没有采取MCST+PRM的技术路线
deepseek 和 kimi 的核心思路是一样的：**关注推理的中间过程是否正确无法实现，所以只能 rule-based reward**，最起码 reward 一定是准的！deepseek 反驳 prm 路线的三个理由是：
1. 定义一个 fine-grain step 很困难；
2. 很难确定一个 step 是否正确，机器标不准，人标无法 scaling up；
    1. 这里，我最认同的是第二点：无法 scaling。假设我们能雇博士生标 10W 条 cot 高质量数据，但能标 100W 条吗？1000W 条呢？就像 scaling law 表达的一样，想让模达到新的效果，需要的数据量级往往是指数增长的。但保不齐以后真的有 scaling prm 数据的方案了，现在一杆子打死为时尚早，也许小模型，或者冷启动用它更好呢？
3. 一旦 PRM 被引入，不可避免的 reward hacking，且训练资源耗费会更多。PS：而MCST方案的核心是提高PRM的准确性，**阻碍MCST的主要是不准的PRM**。Reward是指导MCST搜索方向的重要信号。

在dpsk r1的这篇报告里，提到了2个模型，分别是 DeepSeek-R1-Zero 和 DeepSeek-R1，deepseek R1的想法：把 o1 的训练分为两阶段：step1 学推理，step2 学说话
1. 训 zero 的 step1：全程无标注数据的参与，就是认准了一个目标：让模型的 reward 变高。这个阶段别和我谈模型格式错误逻辑混乱这种细节，我不看模型表现，只看 reward。只不过 reward 变高的过程中，发现模型的输出越来越长了，反思能力也自己涌现出来了；PS：这个阶段主要目的是用来产生比第一次SFT阶段更高质量的推理轨迹COT数据，产生完新的数据zero模型就被抛弃了
2. 基于 zero 训 R1 的 step2：就像是我们做普通的 post training 了，sft 没有被抛弃，除了rule-based reward，reward_model 也被请回来了，reject sampling 也出手了。PS：与step1 差别不是很大，只是sft 数据质量更高了。如此这般，就能形成更通用的多阶段做法。比如我们设计4个阶段，第一阶段类似R1的Phrase 1，得到Model RL-1，使用它产生更高质量的推理轨迹COT数据，然后用这些数据对干净的Base模型进行SFT，开启Phrase 2，得到Model RL-2，由Model RL-2产生质量更进一步的推理轨迹COT数据，如此重复几次，最后一个阶段采用R1的Phrase 2策略，补充标准Post-Training阶段训练数据，防止对通用能力的灾难遗忘问题。而且，在后续的阶段里，可以逐步增加难题占比，采用类似“课程学习”的思想：先从简单问题学起，逐步增加问题难度。**通过多轮迭代找到高质量的推理轨迹COT数据**。

Kimi 的想法：我还是一步到位吧，在 step1 学推理的过程中，要时刻监控着模型的说话能力是否还正常。为了达到此目标，模型的输出长度，模型对每一个 prompt 的回答准确率等信息，全程都被严格监控。
    1. 我太能理解学霸 K 了，我为啥不敢 rule-based reward 一条路走到黑？不就是因为我一训就崩，输出长度崩，performacne 崩，输出格式崩。我崩的时候会自我怀疑是不是选错方案了，学霸 K 崩了则是通过加训练技巧、改 loss 给救回来了。反观学霸 D，他的思路真的太超前太有魄力了， 别去在乎这些细节，二阶段集中解决。PS：K1.5基本上可以看成R1做法的特例情况。
dpsk直接在32B的基础模型上进行了大规模强化学习训练，然后将结果与蒸馏得到的32B模型进行比较。结果令人深思：蒸馏得到的模型在各项指标上都显著优于直接进行强化学习的模型。

[张俊林：MCST树搜索会是复刻OpenAI O1/O3的有效方法吗](https://mp.weixin.qq.com/s/oJFJjk9zbopmLSbh7QbBjg)对于打造更好的o1模型来说，最重要的是如何获得更多高质量的推理轨迹COT数据。这包括如何获得更多的`<问题，答案>`数据，尤其是有难度的问题。拿到<问题，答案>数据后，如何获得质量越来越高的中间推理轨迹COT数据，这里的质量很可能是根据错误中间步骤占比来衡量的，由此得到高质量的`<问题，推理轨迹COT，答案>`数据。这里的推理轨迹COT，可以是人工产生的，可以是大模型自己生成的，也可以是从其它模型蒸馏来得。获得方法貌似也不重要，重要的是质量是否越来越好，在此前提下，再考虑成本是否足够低。PS：大佬认为 k1.5是R1 的特例，R1 是MCST的特例。

## MCTS

[逻辑推理与决策规划：LLM+MCTS](https://zhuanlan.zhihu.com/p/968362756)
1. 为什么要将 LLM 与 MCTS 结合起来？
2. 为什么 LLM 可以与 MCTS 结合起来？
3. LLM 要如何与 MCTS 有效结合起来？
    1. 在训练过程中，MCTS 可以构造出更高质量的数据（比如`<问题，推理轨迹COT，答案>`）以供 LLM 训练；PS：引导让模型生成多个标签，这些标签对应于搜索所需的具体推理步骤。。在训练过程中，首先使用收集到的提示通过由预训练值模型指导的蒙特卡罗树搜索找到答案。随后，使用生成的问题-答案对来同时训练policy/actor模型和reward模型，迭代地改进该过程。这种方法的失败在于next-token的维度爆炸问题非常严重，在优先探索时间下只能采样一部分路径，这些路径可能是不良的，或者是局部最优的，而相关的细粒度reward模型也很难训练，最终导致policy模型难以迭代改进。
    2. 在推理过程中，LLM 通过与 MCTS 的多步交互与迭代，以时间换正确率。PS：MCTS是解码策略的一种，跟Beam search、Top-k sampling、top-p sampling等策略发挥的作用是一样的，只是后几种都是固定规则，而MCTS 有回溯的能力。具有回溯能力的前提是要有一个很好的PRM。每一个query的推理，都是一个全新的MCTS 树。
    3. 无论是推理还是训练，难点是以一个多大的粒度作为 树节点，一个token肯定不能是一个。此外就是如何得到一个好的PRM。
4. LLM + MCTS 是终极方案么，有何局限性？

OpenAI明确表明 o1的训练借鉴了AlphaGo的强化学习思路，而AlphaGo主要使用了Self-Play和蒙特卡洛搜索（MCTS）。根据围棋的规则，棋子可以在19 X 19的棋盘上选择落点。如果使用暴搜法，那么将有 361！种可能，即使去除其中不合法的情况，可能性仍比宇宙中的原子个数$10^80$高出20个数量级。此时，可以使用MCTS来近似暴搜的结果，MCTS是一种用于决策过程的启发式搜索算法，它包含四个步骤：

![](/public/upload/machine/mcts.jpg)

1. 选择（Selection）: 从根节点开始，根据某种策略（如上置信区间 UCB），选择一个最优的子节点，直到到达一个尚未完全展开或终止的节点。
2. 扩展（Expansion）: 如果所选节点不是终端节点，从该节点中随机选择一个未被访问过的子节点，添加到搜索树中。
3. 模拟（Simulation）: 从新扩展的节点开始，进行随机模拟（也称为“Playout”，通常使用随机策略），直到达到游戏的终局状态。根据终局状态的结果（如胜负或得分），评估该模拟。
4. 反向传播（Backpropagation）: 将模拟的结果反向传播到所有经过的节点，更新各个节点的统计信息（如胜率、访问次数），以便未来的决策可以基于更精确的估计。PS：类似于前人某条路走了xx次，胜率xx，后人可以参考

简单通俗地讲，AlphaGo包括两部分：一个CNN卷积神经网络和MCTS算法，CNN相当于人类的”棋感“系统，它负责根据当前棋盘上的形式给出下一步可能的落点的集合；MCTS相当于理性思考，可以在CNN给出的候选落点集合里找到胜率最高的位置。提升的CNN可以找到更好的落点，MCTS的性能也会提升，MCTS性能的提升的结果反馈给CNN，则CNN的性能会再次提升，整个过程形成正反馈循环。AlphaGo首先在人类棋手的棋谱上进行监督学习，然后自我博弈（Self-Play），即在每一局自我对弈中，**AlphaGo 的不同版本与自己对战**，并根据对局结果更新模型的策略网络和价值网络，通过自我对弈，AlphaGo不再依赖人类棋谱，能够探索新的战术和策略，超越人类棋手的水平。

[OpenAI o1模型的前世今生](https://mp.weixin.qq.com/s/OCgbffOPrZ5kzFKisSUC9Q)MCTS多在Action为闭集的场景下使用。笔者认为o1没有使用MCTS，甚至没有使用搜索算法，而是使用类似STaR方法中的。其实MCTS在除了下棋以外的场景上基本没有成功的案例，尤其是在自然语言组成的几乎无限空间的场景上。PS： MCTS主要解决局部最优问题（llm推理时的token选择是固定规则，比如sampling，Best-of-N），从作用看，mcts与sampling和Best-of-N是并列的，认为MCTS是Tree Search的一种。 

[阿里Marco-o1推理大模型技术报告解读](https://mp.weixin.qq.com/s/lqB1elzslAYvlN03bOv7qw)由论文内容推理MTCS和LLM的算法流程，具体来说：
1. 在选择阶段，从根节点开始，算法根据特定的策略（如UCT策略）导航到最有潜力的子节点，直到到达叶节点。
    1. 节点的选择基于其累计奖励（置信度得分）和访问次数，优先选择奖励较高的路径。具体选择标准是从当前节点出发的推理步骤中，挑选出奖励分数  较高的节点，即具有较高累计置信度的路径。
2. 在扩展阶段，在叶节点处，除非它代表游戏的终止状态，否则**会添加一个或多个新的子节点来表示未来的可能移动**。
    1. 在选择阶段到达一个尚未完全展开的节点后，将其子节点扩展到搜索树中。从当前节点输入 LLM，生成潜在的下一步推理输出，这些输出被作为新节点添加到搜索树。生成的子节点代表不同的推理方向（例如下一步的逻辑步骤或不同的解法路径）。扩展过程通常生成多个候选输出，例如基于 LLM 前 5 个置信度最高的候选 token，从而捕获可能的推理分支。
3. 在模拟阶段，从新添加的节点开始，算法进行随机模拟（通常称为“rollouts”），选择随机移动直到游戏结束，从而评估节点的潜力。
    1. 模拟由 LLM 完成，执行一个完整的推理链展开。在每一步中，LLM生成的 token 置信度通过 softmax 计算得出。整个模拟路径的总体奖励v是所有 token 置信度得分的平均值。模拟结束的条件可以是推理链完成、达到预设长度或模型预测的终止标记。
4. 在回溯更新阶段，模拟结束后，结果（赢、输或平局）被反向传播回根节点，更新每个遍历节点的统计数据（如赢、输次数），以指导未来的决策。
    1. 将模拟结果（奖励v）沿路径从当前节点反向传播，更新父节点及祖先节点的统计信息。每个节点更新其累计奖励和访问次数：奖励：$w_i <- w_i + v$ ，访问次数：$n_i <- n_i +1$ 。奖励值v是 roll-out 路径的置信度得分的平均值，表示路径推理质量。回溯更新确保整个搜索树能够动态调整，未来的搜索倾向于奖励较高的路径。PS：有一个推理llm （带上cot？）生成一个推理路径，有一个对推理路径评分的模型，进而优化推理llm的参数。 MTCS的用处是扩展了搜索空间，呈现类似TOT的效果
通过反复迭代这些阶段，MCTS逐步构建决策树，优化在状态空间庞大且难以直接计算最佳策略的场景中的决策。

推理动作策略。
1. 动作选择。研究者观察到，以动作作为蒙特卡洛树搜索（MCTS）的粒度较为粗糙，这种方式往往导致模型忽视解决复杂问题所需的关键推理路径。为了解决这一问题，研究者尝试了不同粒度的**搜索单元**。起初，研究者使用完整的“步骤”作为搜索的基本单位。随后，为了进一步扩展模型的搜索空间并提升其解决问题的能力，研究者尝试将步骤细化为更小的单元，即每32或64个Token组成的“微步骤（mini-step）”。这种更细粒度的划分使模型能够以更高的精度探索推理路径。尽管在理论上，以Token级别为单位的搜索能够提供最大的灵活性和精细化，但由于计算资源的高昂需求以及构建有效奖励模型的难度，目前在实践中尚不可行。在实验中，研究者在MCTS框架下实现了以下策略：
    1. 步骤（Step）作为动作（Action）：允许模型生成完整的推理步骤作为动作，每个MCTS节点代表一个完整的思维过程或行动标签。这种方法在探索效率上具有优势，但可能忽略解决复杂问题时需要的更细粒度的推理路径。
    2. 微步骤（Mini-step）作为动作（Action）：将32或64个Token作为一个微步骤的动作单元。这种更细的粒度扩大了问题解决的空间，通过在搜索过程中引入更多细微步骤，增强了模型应对复杂推理任务的能力。在这一粒度水平上探索解决方案空间，能够帮助模型发现以较大动作单元可能忽略的正确答案。
    研究结果表明，采用更细粒度的MCTS搜索策略可以显著提升模型的推理能力，从而更有效地解决复杂问题。
2. 反思机制。研究者引入了一种反思机制，通过在每次推理过程末尾添加短语“等等！也许我犯了一些错误！我需要从头开始重新思考。”来促使模型进行自我反思并重新评估推理步骤。这种机制显著提高了解题的准确性，特别是在原始模型初始错误解决的复杂问题上表现尤为突出。加入反思机制后，大约有一半此类困难问题能够被正确解决。从自我批评的角度来看，这一方法使模型能够充当自己的批评者，从而识别推理中的潜在错误。通过明确提示模型质疑其初始结论，这一机制鼓励模型重新表达并优化其思维过程。自我批评机制充分利用了模型检测自身输出中不一致性或错误的能力，从而提升了解题的准确性和可靠性。反思步骤作为一个内部反馈循环，有效增强了模型在没有外部干预情况下的自我纠错能力。这一机制不仅显著提升了模型解决复杂问题的能力，还强化了其解题的准确性和鲁棒性。

[MCT Self-Refine (MCTSr)的算法（包含代码理解）](https://mp.weixin.qq.com/s/6hFxdtenAsJ0qU1wEP_XOA) 代码层级的理解看这里。

## 从人类决策习惯来看

[如何提升大模型的“深度思维能力”](https://mp.weixin.qq.com/s/lSmPkq10Ao423DujutEZ1A)
人类的思维过程始终在动态平衡中运行：一方面，通过联想、查阅资料、与他人交流等方式“增加”信息；另一方面，通过筛选、排除、归纳等方式“减少”信息，逐步聚焦于可行的选项。这种“增-减”的循环贯穿于大部分推理和决策。从信息论的角度看，人类决策的核心目标是降低信息熵。信息熵代表了系统的不确定性程度。在高熵情况下，所有选项看似差异不大，决策难以推进；在低熵情况下，选项变得更加明确，决策变得轻松。以旅游计划为例，假如我们同时考虑了数十个景点，而它们在吸引力、距离等方面差异不大，就会产生高信息熵的困境。为了突破这一困境，人类通常会引入更多信息，如“当前季节的景色特点”“附近是否有知名餐厅”等，以逐步拉开选项间的差距，最终让决策变得明确。

既然人类是通过增-减信息以降低信息熵的，那么同样的模式能否应用到大模型上呢？我们先观察信息熵是否对大模型而言同样适用。大模型通过预测下一个词生成文本：
1. 当下一个词非常确定，则显然的概率被集中在少数甚至一个答案上，这时其信息熵较低，模型对于结果比较有信心。
2. 当下一个词的概率分布高度分散、每个词看起来几乎等概率时，也是信息熵极高的情况，模型实际上处于一种无法判断的状态。需要注意的是，这种“无法判断”不是简单地回答“我不知道”，而是连是否该选择“我不知道”都不确定。
3. 从结果看，信息熵能代表模型的准度，尽管这一准度的评委似乎是模型自己，这本身似乎陷入了“循环论证”的过程，然而大模型本身也是对人类逻辑的拟合，实际上人类也在试图“解释”现象以降低信息熵，提高一致性，数据的一致性越高，模型拟合到的推理过程的一致性就越高。我们暂且搁置实际执行过程中可能存在的“循环论证”的问题，转而做一些观察，我们看看增-减信息以降低信息熵的方式，能否让模型本身准度有所提高：
4. 对于增加论据的模式，COT应用的成功也证明了增加相关“证据”并逐渐接近答案，能让模型的准确率显著提高，增加足够多的论述过程，会让模型更倾向于得到这些论据所论述的答案。
5. 对于减少噪声的模式，可以观察到的一点是，尽管拥有强大的注意力机制，但当上下文过长或信息过于复杂时，模型的预测精度也会显著下降。适时总结前文继续往下推理的模式，也是能让模型精度有所提升的。

根据之前探讨的内容，模型的语料中缺乏思维能力，甚至有很多相互矛盾的内容，这样的语料内在逻辑并不自洽，我们会尝试探讨如何像人一样重新“思考”一遍这些语料，并把思考过程的信息补充进去，以得到一个逻辑自洽的语料，从而降低信息熵，增加模型准确度，并且使得模型具备看上去的“思维能力”。
我们需要对于符号做一些简单的约定，我们用大写字母表示“证据/结论”：
1. 推导过程用“->”符号表示，例如“杭州->适合旅游”，这意味着模型看到杭州这个词推导出适合旅游这个结论。
2. 共同作用的推导过程“()->”表示，例如（充足的阳光，定期浇水）->植物健康生长，同理更多的条件时以(A,B,C,D)->E表示。

**在增加信息以获取更强大模型能力的角度看**，多步推理是一个非常重要的场景，典型的形式是A → B → C → D的链式推理。然而，现实语料中，面对复杂问题我们往往看到A → D的简化结论，例如“如何评价xxx公司的未来发展”，大模型往往倾向于直接“背诵”已有的分析结果，这是因为语料中对于这类复杂问题更多是结果性的内容，推理路径的缺失成为模型学习复杂逻辑的主要障碍。这引出了一个核心问题：**如何合成逻辑自洽的推理路径，提升大模型的思维能力？**
1. 路径生成。生成推理路径的难点在于路径的多样性与逻辑自洽。对于同一个问题，可能存在多条不同的合理解法。以数学为例，一个定理往往可以通过多种证明方式得到结果；而在自然语言中，推理链条的表达可能更加多样化，语义上的灵活性让问题更加复杂。在数学问题上，CoT技术是非常成功的应用，通过CoT产生更多的中间推理过程数据，从而用以训练能使得模型可以逐步生成逻辑清晰的中间步骤，直到最终答案。这其中很关键的两个要素在于下一个论据的可枚举以和可验证，对于一般任务而言任务，我们可以采取更宽松的方式，例如通过大模型本身（或专为此任务微调的模型）结合人工定制策略扩充推理数据，另一种思路是直接借鉴已有人工设计的推理链条，尤其是在特定领域已经实践的人工编排策略，这种方法可以快速生成高质量的推理语料。
2. 推理粒度与压缩。生成多步推理路径的另一个关键问题是：在什么情况下直接从A → D更有效？什么时候需要完整的A → B → C → D路径？这实际上涉及到语言模型的压缩原理。语言模型的本质是学习词与词、句与句之间的关系，并将这些知识压缩进模型。如果在语料中，A → D的频次足够高，模型学习这一关系的代价会远低于生成A → B、B → C、C → D三个关系之和。此时，从“压缩效率”的角度看，直接记忆A → D更优。然而，当A → D的出现频率较低，或者需要推理的任务复杂性更高时，模型会倾向于依赖A → B → C → D等逐步推理过程。从人类的角度来看，这种现象也有迹可循。我们的深度思考（“System 2”）会将高频推理结论沉淀为直觉（“System 1”）。例如，在日常生活中，我们很少再去逐步推导“重力会让物体下落”这一结论，而是直接将其作为常识。对于大模型，类似的压缩机制也可能自然发生，尤其是在A → D的频率足够高时。一个关键的启发是，“论据”粒度的选择需要基于全局视角。对于高频结论，直接学习A → D是一种高效选择，因为它减少了推理链条的存储和计算开销。对于低频结论或复杂任务，生成完整路径A → B → C → D则显得尤为重要。这不仅能提升模型的泛化能力，还能为任务提供更高的可解释性。这种粒度选择需要结合语料的统计分布和推理任务的目标。理想情况下，模型应能够根据任务需求动态调整推理路径的粒度：在需要效率时，优先使用简化结论；在需要逻辑完备时，生成详细路径。
3. 多结论问题。在数据中的表现为同时存在A → B和A → C的语料，当模型学这种语料中学习时，遇到A为前提的问题，无法判断答案应该是B还是C，亦或者错误答案C的语料多，而正确答案B的语料少，在缺乏更多信息的情况下，模型很难给出正确答案。这种情况下，真实逻辑可能的是：(A, D) → B 和 (A, E) → C，也就是说，隐藏的条件D或E对推理结果起到了决定性作用。这种现象实际上颇为常见，例如同样是给出行程安排的情况，一些猎奇博主给出的旅行方案可能全篇都是猎奇的，而美食博主则可能更多关注美食，如果能把贯穿始终的风格提前识别出来并注入语料，能让模型更多以更一致的视角回答问题，降低信息熵，提高模型准度。对于一般的“错误和偏见”问题，例如“三亚这人虽然不多，但是太热了，不适合来玩”，可能是因为“现在是夏天，而夏天是淡季”，这种类似人类阅读直觉的，能让语料本身的逻辑更加自洽。除此之外类似小说创作先列出大纲，笑话先写出笑点，对于创造的稳定性而言都是显著的增加。
4. 降低信息/噪声。在人类的逻辑语言中，经常在同一篇文章中会存在大量不相关的内容，越是长的论述中，越容易出现这种现象，因为人类在做深度思维时，总会习惯性的拆分出多个子问题，以分开论述，例如(A, B) → C，在逐步证明A和B的过程中，会引入大量的过程论述，一直递归到足以回答问题为止。然而这一过程也为整个文本注入了大量的噪声，例如当我们论述B时，A的论述过程对B而言是噪声。类似的现象还有很多，例如当我们让大模型帮我们写一本小说时，我们会列好大纲，也可能是多级的大纲，然而当我们考虑把大纲中某一章节展开细节时，前文大量的过程都我们而言都是噪音，亦或是大纲的形态也可能是双线叙事，另一条故事线的很多细节也是对于本故事线而言也是噪音，我们只会关注交织点或者可能的共同点等。

可以看到，不管是增加信息还是减少信息，其过程都对语料全局的统计信息有所依赖，例如单步推理的合理性评估、对文本目的性的识别、路径颗粒度的合理程度等等，对于统计信息的依赖，一个比较容易的路径就是利用现有的大模型，因为现有大模型已经对整体语料“学习”过一遍，再对其进行针对性微调，模型在很大程度上能胜任这些任务。也就是说，**通过当前模型的弱推理、弱规划能力，逐步强化数据以达到更强的推理、规划能力**。
对于模型如何学习“人类思考过程”还有很多可以探讨的内容，限于篇幅，太多细节不便展开。人类在“阅读”知识的时候，会持续加入自身理解和判断，主动聚焦和拆解问题，而**目前大部分语料并不具备这样的过程记录**。对于大模型而言，通过对人类学习过程的模拟来加深模型的思考能力，能很大程度提高其推理能力。深度思考能力是人类解决复杂问题能力的核心，甚至是通往AGI的必经之路，**通过预训练语料的修正只是其中一环**，如何与真实世界链接，如何建立类人的反馈机制，这些问题一步步解决，似乎让我们能够瞥见AGI一眼。
PS：让llm有思维能力，一个是预训练 数据就得有体现思维的数据让llm“见识”过，再一个就是微调的时候对推理过程有评价，让llm被“知错就改、刻意练习”过。

ps： 梳理下，如果要提高llm推理能力，post-trainning时就不能只靠`<问题，答案>`，而要靠`<问题，推理轨迹COT，答案>`，让llm可以输出步骤，进而通过rl（也就是要有一个rm或类似工具）来评价这些步骤，进而更新llm参数。
1. `<问题，推理轨迹COT，答案>` 从哪里来，如何过滤带有错误步骤、错误答案的路径，进而sft llm。这个阶段的主要目的一个是让大模型能够产生初步的深度思考能力，一个是输出格式满足我们希望的形式，通过调整模型参数让这种能力内化到模型里。
2. 如何rl？比如我们现在拿到一个问题以及对应的标准答案数据<问题，标准答案>，此时让Model-SFT自己产生K条完整的推理轨迹COT，以及对应的答案。   
    1. ORM（结果回报模型）根据某条推理轨迹产生的答案是否和问题的标准答案相同给出Reward，符合标准答案给出正向高回报，不符合标准答案则负向低回报。同一个问题，K个推理轨迹，K个答案，产生高低不同的K个回报。我们可以认为得到高回报的推理轨迹质量高，于是，我们希望根据目前得到的这些数据，来调整下模型参数，让模型以后倾向输出那些高回报的推理轨迹COT作为思考过程，而别产生那些低回报的推理轨迹COT，这是强化学习希望达到的目标。PS: 结果对+ 格式对，就算你推理对？
    2. MCTS+ PRM 基于每个step进行过程评价
3. 高质量 `<问题，推理轨迹COT，答案>` 不是一步到位的，一般要经过一个多轮迭代的过程。



