---

layout: post
title: deepresearch梳理
category: 技术
tags: MachineLearning
keywords: deepresearch deepsearch

---

* TOC
{:toc}

## 简介

什么是Deep Research? 它是一个深度搜索和调研的Agent，能在5-30分钟内出一份完整的调研报告。是一个典型的垂直 Agent 场景，不属于通用 Agent 的范畴。注意，它强调"深度搜索+调研"，而非单纯的深度搜索(Deep Search)。

1. 与Deep Search关系，要复现Deep Research，首先要把搜索(Search)做好。给定用户问题，Agent要学会从浏览器或API中搜集相关知识。没有扎实的Search能力，就难以实现Research。
2. Deep Search（深度思考）：更侧重“找”的过程，它的目标是把相关信息尽可能全面、深入地挖掘出来。Deep ReSearch更侧重“写”的过程，它可以利用Deep Search找到的信息，进行深入地思考，分析和创造，目标是产出深刻的理解或知识。
3. 传统RAG，是当用户提出问题，系统会从大规模的知识库（通常是向量检索库）中检索出跟问题最相关的文档片段或信息，然后将这些检索到的信息连同原始问题一起提供给LLM，让LLM基于这些“增强”的上下文信息生成更准确的、更具事实性的答案。RAG中的llm比较被动，尤其是一轮rag场景仅用于生成答案（不参与检索）

一份好的深度研究报告，倒也不一定非得是万字长文，它需要搞定几件事：
1. 能够充分理解问题，并给出一份结构合理的框架。
2. 能找到真正有价值的信源，而不是无关紧要的信息。PS：所以得过滤低质或冗余信息
3. 最终生成的内容，能够围绕核心问题展开。

## Deep Search：搜索的本质与难点

高质量的信息，是 Agent 做所有事情的起点。如果一个 Agent，连问题的来龙去脉都搞不清楚，那它怎么可能规划出来靠谱的执行路径？最近圈子里又在谈提示词，有种观点认为提示词工程应该叫 Context 工程。概念不重要，重要的是大家意识到 Context 是 Agent 能够高效准确执行任务的关键。而我觉得，**深度研究就是在为 AI 准备一个高质量的 Context**。

[端到端的训练，怎么复现 Deep ReSearch（上） ：先从 Deep Search 做起](https://zhuanlan.zhihu.com/p/1892489650469323191)

多跳搜索和深度研究型搜索的关键在于模仿人的思维链搜索： 
1. 模型首先根据问题进行初步推理，确定基础搜索方向
2. 执行初始搜索，获取第一批信息
3. 基于已获取的信息，进行下一轮推理，确定进一步的搜索方向
4. 执行细化搜索，获取更精准的信息
5. 不断迭代这个"推理→搜索→推理"的循环，直到收集足够信息 在这个过程中，每次搜索都建立在前一次搜索结果的基础上，形成一个连贯的推理链。

传统 RAG 通常是一次性的：在回答问题前进行一次检索，将检索结果放入上下文中。针对动态的、多步骤的检索机制有一些论文
1. Search-o1 是最近比较火的 WebThinker 项目的前身，
    1. 模型在推理过程中可以识别自身知识的不足点，当遇到知识不确定的情况时，模型会自动生成搜索查询，格式为` <|begin_search_query|>搜索词<|end_search_query|>`
    2. 系统检测到这一标记后，暂停模型推理，执行网络搜索
    3. Reason-in-Documents 模块分析搜索结果，提取关键信息
    4. 精炼后的内容被包装在 `<|begin_search_result|>提炼后的检索内容<|end_search_result|>` 中
    5. 模型继续推理，可能进行多轮搜索-精炼循环
    6. 最终生成完整且准确的答案 !
2. DeepRetrieval，用强化学习来训练query改写，奖励函数设计值得一看。
3. Search-R1，Search-R1没有 Search-o1的 Reason-in-Documents模块，检索到的内容是直接完整放到思维链中的。Search-R1针对任务进行了强化学习的训练，使用基于规则的奖励系统，只关注最终结果的正确性
    1. 当接收到用户问题时，模型首先在`<think>`思考标签内进行初步推理分析，识别当前知识储备中的信息缺口。
    2. 若推理过程中发现知识不足，模型将自主触发检索机制，通过`<search>查询内容</search>`格式生成精准搜索指令。
    3. 搜索引擎返回的结果会被结构化封装在`<information>`信息标签内，为后续推理提供可靠的外部知识输入。
    4. 系统支持多轮次检索-推理循环，模型可根据信息完备性动态决定是否发起新一轮检索，直至满足解答需求。
    5. 当判定信息充足时，模型直接通过`<answer>`答案标签输出简洁结论，无需附加解释说明。
4. R1-Searcher，引入了一个两阶段基于结果的强化学习方法，使LLM能够在推理过程中自主调用外部搜索系统
    1. 第一阶段(检索学习训练)：通过检索奖励激励模型学习如何正确调用外部搜索，不关注答案准确性。
    2. 第二阶段(检索结果集成训练)：在确保格式规范的基础上，引入了答案奖励，提升模型有效利用检索信息解决问题的能力

[Deepresearch核心技术：如何通过强化学习增强推理大模型搜索规划及反馈能力？](https://mp.weixin.qq.com/s/2zlrVmiXHJ6bv12YLD68FA) 未细读。

## 能真正留住用户的，一定是"Deep"。

[端到端的训练，怎么复现 Deep ReSearch（中） ：围绕着"Deep"，解构 Jina 项目的实现](https://zhuanlan.zhihu.com/p/1898295379990132543)
为什么要聚焦于"Deep"？因为它是Deep Research的核心价值。当用户愿意等待5-30分钟来获得一份研究报告，他们期待的必然是高质量的内容，如果最终报告无法让用户产生"啊哈"时刻，无法让他们在某些时刻惊叹"这报告真厉害""这报告部分内容写得比我还好""考虑得比我还全面"，那么这款应用很可能注定失败。

什么是真正的"Deep"？
1. 内容篇幅充足。内容过少往往意味着分析不够全面深入，当然也不能过长，过长的内容可能给用户带来阅读压力。
2. 描述具体且有洞察力：比如分析"人工智能对就业市场的影响"时，浅层表达是"AI将替代一些工作岗位"，而深度表达是"AI将重塑金融行业的职业结构，替代初级分析师的数据处理工作，同时创造数据伦理专家和AI-人类协作经理等新岗位，预计到2026年净就业影响为正增长12%"。
3. 引用权威且适当的资料：专业分析报告通常会引用大量权威资料佐证观点。如果你的Deep Research缺乏引用或引用了不可靠来源，将很难让用户信服。

在复现Deep Research时，请始终记住目标是让你的agent变得更"Deep"，围绕这这个目标去构建工作流、调整 prompt 、微调等等等等。

1. 对问题建立对应的评估标准，包含四种核心评估维度：

    1. Definitive（明确性）：判断问题是否需要明确的答案，而非模糊表述。例如："谁发明了微积分？"—需要明确的答案
    2. Freshness（时效性）：判断问题是否需要最新信息。例如："当前美国的利率是多少？"—需要最新经济数据
    3. Plurality（多样性）：判断问题是否需要多个项目或示例。例如："请列出五个最著名的莎士比亚悲剧"—需要列出多个项目
    4. Completeness（完整性）：判断问题是否包含多个需要全面解答的元素。例如："赤壁之战的历史背景、参与者、战略意义及影响是什么？"—需要全面回答多个方面
    这些评估标准背后各自对应不同的Prompt。不同的问题使用不同的评估标准。 这些评估标准将在最终答案生成后使用，即在得到答案之后，系统会通过这些评估标准来检查答案。
2. Jina将深度研究抽象为四个核心动作（action）的不断选择。具体来说，系统会让LLM基于当前的记忆（包括之前的步骤和已获取的知识）通过Prompt来决定采取哪一个核心动作。这四个核心动作分别是：搜索（Search）、阅读（Visit）、思考（Reflect）和回答（Answer）。**每个动作背后都有复杂的实现机制和精细的工程思考**。
    ![](/public/upload/machine/jina_deepsearch.jpg)
    构建了一个循环的任务处理流程，其中维护一个 gaps 问题列表（可以看成是一个动态的执行计划），在一个由模型自主判断的处理闭环中，每个 step，系统从gaps 列表中抽取一个问题，每个 step 由模型自主根据上下文信息，采取几种 action之一。这样一步步循环推进，直到 gaps 列表清空，或达到 token 限制。整个过程中，依靠系统 system message 和 knowledge message 来进行上下文管理。

## 端到端 vs 固定工作流：两者的折中方案

真正端到端形态的，长的像是豆包目前的“深度思考”功能，如下图。豆包的深度思考实现了“边想边搜”的能力。在一条单一的思维链中，模型一边推理一边发起搜索请求，只需要维护一条持续生长的长链即可。

![](/public/upload/machine/doubao_deepsearch.jpg)

目前不少厂商推出的Deep Research功能，其实更偏向于端到端和固定工作流的中间形态。这些系统会先将任务抽象成若干个action，再依据预设的 plan 和上下文记忆，让llm自己决定当前step应该采取的 action。从架构上看，这其实更像是“多轮对话系统”——每个 step 类似于一轮对话，由模型决定下一步要采取的行动。

不过，从工程实践的角度，这种“折中式”方案反而更加可控和高效，原因有以下几点：

1. 提高效率与资源利用率：将任务拆解后，不同模型可以分工协作，各司其职。
    1. 例如：专门针对规划任务训练过的小模型负责规划，推理模型负责处理复杂推理，擅长写作的模型负责总结写作等生成任务。就像一个项目团队，有实习生、初级工程师和高级工程师，要把任务拆分，协同作业，更容易做成一个 big project，而不是把所有活都交给最厉害的人干，**一个人干完当然可以，但再厉害的人也无法面面兼顾，且会效率低下**。
    2. OpenAI 的 CPO Kevin Weil 在近期的一次播客中就提到，好的系统设计更倾向于多模型协同，“All-in-one”并不一定是最优解
2. 适当抽象和任务拆分，适合 scaling：**通过将任务中常见操作抽象为有限的 action**（例如：调用外部工具、读取结果、反思规划、尝试回答等），模型只需根据上下文动态选择合适的 action。这种抽象之后，系统的可扩展性大大提升
    1. 在广度上：只需新增各类 MCP（工具接口），如文献搜索、Google 查询、Markdown 自动生成等，就能不断拓展系统能力。
    2. 在深度上：可以通过强制执行多轮反思、设定 token 使用下限、对回答进行不同维度的评估等策略，确保模型深入思考、避免浮于表面。
这种机制下，我们更推崇的是一种理念：Less Control, More Tools/MCPs（不是 0 控制，而是适度控制 + 工具赋能）。

### 端到端 

[月之暗面 Kimi 首个 Agent 开启内测，可生成易追溯的万字报告，有哪些技术亮点？](https://www.zhihu.com/question/1919712376204256921/answer/1920925901035644513)端到端强化学习（RL）的优势：让模型自己“进化”
1. 挣脱“固定流程”的束缚，更灵活通用。RL Agent的行为不是被规则写死的，而是根据当前任务动态生成的。这让它在面对闻所未闻的复杂问题时，有能力探索出创造性的解决方案。我们升级底层模型时，也无需重构整个Agent体系。
2. 能力上限更高，用“数据”而非“设计”来驱动增长 当我们发现Agent在某类问题上表现不佳时，我们的解决方案不是去绞尽脑汁地修改Prompt或Workflow，而是将这类问题加入到训练数据中，**通过增加“训练题量”和算力，让模型自己学会如何解决**。前者的天花板是“人的智慧”，后者的天花板是“数据和算力”——我们坚信后者要高得多。PS: 堆规则 ==> 堆数据。
3. 能Scale。相比SFT依赖人类标注，RL路线可以让Agent在环境中不断探索，只要我们能准确判断任务是否成功（即提供准确的奖励信号），加大算力去Rollout，就能获得源源不断的、高质量的on-policy训练数据，让模型持续不断地自我迭代和提升。（感兴趣的同学可以去读下The Bitter Lesson）

## 模型怎么训练

### 怎么奖励？

现在的主流训练范式，“推理 + 强化学习（RL）”已经跑通了，只要我们能定义好如何在与环境交互时设计合适的奖励机制（当我们学会这个问题怎么定义怎么评估时，其实我们就多多少少知道奖励该怎么设计），用这种范式训练模型后就能让效果直接拔高。然而，问题也随之而来：“Deep Research”这样的复杂应用为例，**奖励到底应该如何设计呢？**这一点并不简单。举个例子，现在有两个报告：
1. 报告 1 写作非常流畅、逻辑严谨，但引用不够丰富。
2. 报告 2 写作较差，但引用信息丰富且权威。

我们应该给予哪份报告更多的奖励？对于 Deep Research 这样的任务，我们需要更细粒度的评估标准，比如引用是否合理多样、报告中的公式计算是否准确、是否能够挖掘长尾信息等等等等。并不是简单地给报告打一个0到10分的奖励就能解决问题。


### 数据

OpenAI 的 Deep Research 在他们的 System card 中展示了训练数据和方法。他们的训练数据集涵盖了从有标准答案的“objective auto-gradable tasks with ground truth answers”到“more open-ended tasks with accompanying rubrics for grading”。在训练过程中，他们使用思维链模型作为评分器，将模型的输出与标准答案或评分标准进行对比评分。这意味着，openai的训练数据既包括像简单的多跳问答这种有固定答案的任务，也包括更加开放式的写作任务。字节最近发布的一篇论文《Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback》也讨论了RL 的训练问题。论文指出，基于规则的设计更容易帮助模型学习到 fine-grained 的信息，不容易遭受“reward hacking”。而纯开放式任务则可能被模型“钻空子”，只学会表面技巧，忽视了任务的深层价值，因此只能学习到coarse grained信息。DeepSeek-R1 的最后强化学习阶段，也是通过混合使用基于规则的奖励和开放的奖励模型来进行训练。因此，目前的建议是：最好结合有标准答案的任务（做 rule-based reward）与开放式任务（要专门训一个reward model来奖励），将这两种类型的奖励混合使用。除此之外，其他的训练策略或许还需要社区更多的研究工作。PS: 就是你不要用单一目标来训练模型

为什么开源Agent在解决真正复杂的难题时，总是被OpenAI的DeepResearch按在地上摩擦？阿里通义开源了他们最新的Web Agent模型（还有代码、论文）——WebSailor。论文指出，**问题出在训练数据的难度上**（PS：数据仍然是Agent时代的护城河）。之前的训练方法，基本都围绕着两类任务：
- Level 1: 低不确定性任务，比如单次搜索就能找到答案的问题。
- Level 2: 路径明确的多跳任务，比如“阿里巴巴现任CEO的母校的第一位中科院院士是谁？”。虽然复杂，但推理路径是固定的、线性的。
然而，现实中，很多挑战，属于 Level 3 ：极高的不确定性 + 极其复杂的探索路径。它没有标准答案路径，需要Agent像一个真正的研究员一样，在信息的海洋里不断探索、剪枝、整合、推理。**用Level 1和Level 2的数据去训练模型，然后让它去解决Level 3的问题，这无异于只教了加减法，就让学生去解微积分**。结果自然是惨不忍睹。有哪些秘诀呢？
1. 构造出L3级别的合成数据
    1. 构建复杂知识图谱：从真实世界的网站出发，通过随机游走的方式，构建出一个包含大量实体和复杂关系的高度互联的知识图谱。这保证了问题的源头是真实的，结构是非线性的。
    2. 采样+提问：从这个复杂的图中，随机采样出一个子图，然后基于这个子图生成问题和答案。
    3. 制造难度（关键步骤）：在生成问题时，故意对信息进行模糊化处理。这招太绝了。
        1. 精确的日期，变成 “21世纪初”。
        2. 清晰的名字，变成 “一个由F开头的人创立的机构”。
        3. 具体的数值，变成 “市场份额不到1%”。
        这种MASK直接把任务的初始不确定性拉满，逼着Agent必须学会比较、推理、综合信息，而不是简单地执行查找。
2. 不学废话，只学精华。**有了高质量的QA，下一步就是生成解题过程的轨迹，让模型去学习**。传统方法是找一个更强的专家模型（比如QwQ-32B），让它生成完整的思考和动作轨迹，然后让我们的模型去模仿。但这里有个大坑：专家模型通常非常啰嗦！它们的思考过程充满了冗长、风格化的“废话”。直接学习这些，不仅会污染我们模型的思考风格，限制其灵活性，更致命的是，在需要几十步工具调用的长任务里，这些废话很快就会把上下文窗口（Context）撑爆！WebSailor的做法堪称教科书级的取其精华，去其糟粕：
    1. 让专家模型生成完整轨迹，但只保留action-observation序列。这相当于只看大师的操作，不听他的碎碎念。
    2. 然后，再用另一个强大的指令跟随模型，去为每一步成功的动作反向生成一个简洁、凝练、直指目标的“思考”。
    这样得到的训练轨迹，既保留了专家解决问题的核心逻辑，又干净利落，没有废话，非常适合长任务的训练。
3. 先冷启动，再用DUPO精调。最后是训练环节。WebSailor采用了“两步走”策略。
    1. RFT冷启动。他们发现，直接上RL（强化学习）效果很差，因为任务太难，奖励太稀疏，模型一开始根本不知道往哪走。所以，需要先用少量（仅2k）经过筛选的高质量SFT数据进行“冷启动”，让模型先掌握基本的工具使用和长链条推理的“骨架”。
    2. DUPO算法强化。Duplicating Sampling Policy Optimization (DUPO)。相比之前的DAPO等方法，它最大的优势是快。在Agent的RL训练中，与环境交互的“rollout”过程非常耗时。DUPO通过一个聪明的技巧——在训练中，优先复制（duplicate）那些表现出多样性（部分rollout成功，部分失败）的样本来填满一个batch，而不是去环境中拉取新样本——极大地提升了训练效率，实现了约2-3倍的加速。


