---

layout: post
title: deepresearch梳理
category: 技术
tags: MachineLearning
keywords: deepresearch deepsearch

---

* TOC
{:toc}

## 简介（未完成）

什么是Deep Research? 它是一个深度搜索和调研的Agent，能在5-30分钟内出一份完整的调研报告。注意,它强调"深度搜索+调研"，而非单纯的深度搜索(Deep Search)。

1. 与Deep Search关系，要复现Deep Research，首先要把搜索(Search)做好。给定用户问题，Agent要学会从浏览器或API中搜集相关知识。没有扎实的Search能力，就难以实现Research。

## Deep Search：搜索的本质与难点

[端到端的训练，怎么复现 Deep ReSearch（上） ：先从 Deep Search 做起](https://zhuanlan.zhihu.com/p/1892489650469323191)

多跳搜索和深度研究型搜索的关键在于模仿人的思维链搜索： 
1. 模型首先根据问题进行初步推理，确定基础搜索方向
2. 执行初始搜索，获取第一批信息
3. 基于已获取的信息，进行下一轮推理，确定进一步的搜索方向
4. 执行细化搜索，获取更精准的信息
5. 不断迭代这个"推理→搜索→推理"的循环，直到收集足够信息 在这个过程中，每次搜索都建立在前一次搜索结果的基础上，形成一个连贯的推理链。

传统 RAG 通常是一次性的：在回答问题前进行一次检索，将检索结果放入上下文中。针对动态的、多步骤的检索机制有一些论文
1. Search-o1 是最近比较火的 WebThinker 项目的前身，
    1. 模型在推理过程中可以识别自身知识的不足点，当遇到知识不确定的情况时，模型会自动生成搜索查询，格式为` <|begin_search_query|>搜索词<|end_search_query|>`
    2. 系统检测到这一标记后，暂停模型推理，执行网络搜索
    3. Reason-in-Documents 模块分析搜索结果，提取关键信息
    4. 精炼后的内容被包装在 `<|begin_search_result|>提炼后的检索内容<|end_search_result|>` 中
    5. 模型继续推理，可能进行多轮搜索-精炼循环
    6. 最终生成完整且准确的答案 !
2. DeepRetrieval，用强化学习来训练query改写，奖励函数设计值得一看。
3. Search-R1，Search-R1没有 Search-o1的 Reason-in-Documents模块，检索到的内容是直接完整放到思维链中的。Search-R1针对任务进行了强化学习的训练，使用基于规则的奖励系统，只关注最终结果的正确性
    1. 当接收到用户问题时，模型首先在`<think>`思考标签内进行初步推理分析，识别当前知识储备中的信息缺口。
    2. 若推理过程中发现知识不足，模型将自主触发检索机制，通过`<search>查询内容</search>`格式生成精准搜索指令。
    3. 搜索引擎返回的结果会被结构化封装在`<information>`信息标签内，为后续推理提供可靠的外部知识输入。
    4. 系统支持多轮次检索-推理循环，模型可根据信息完备性动态决定是否发起新一轮检索，直至满足解答需求。
    5. 当判定信息充足时，模型直接通过`<answer>`答案标签输出简洁结论，无需附加解释说明。
4. R1-Searcher，引入了一个两阶段基于结果的强化学习方法，使LLM能够在推理过程中自主调用外部搜索系统
    1. 第一阶段(检索学习训练)：通过检索奖励激励模型学习如何正确调用外部搜索，不关注答案准确性。
    2. 第二阶段(检索结果集成训练)：在确保格式规范的基础上，引入了答案奖励，提升模型有效利用检索信息解决问题的能力

[Deepresearch核心技术：如何通过强化学习增强推理大模型搜索规划及反馈能力？](https://mp.weixin.qq.com/s/2zlrVmiXHJ6bv12YLD68FA) 未细读。