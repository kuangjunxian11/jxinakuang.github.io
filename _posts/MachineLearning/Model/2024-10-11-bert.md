---

layout: post
title: bert
category: 架构
tags: MachineLearning
keywords:  gcn

---

## 简介

* TOC
{:toc}

BERT 是一个用 Transformers 作为特征抽取器的深度双向预训练语言理解模型。通过海量语料预训练，得到序列当前最全面的局部和全局特征表示。

[论文](https://arxiv.org/abs/1810.04805v1) 

bert 名称来自 Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional(相对gpt的单向来说，是双向的) representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. PS：ELMo 是基于rnn，在应用到下游任务时，还需要做一些模型结构的变动。有了一个训练好的bert之后，只需要再加一个额外的层，就可以适配各种任务。

## 模型结构

BERT的基础集成单元是Transformer的Encoder，BERT与Transformer 的编码方式一样。将固定长度的字符串作为输入，数据由下而上传递计算，每一层都用到了self attention，并通过前馈神经网络传递其结果，将其交给下一个编码器。

![](/public/upload/machine/bert_model.jpg)

模型输入

![](/public/upload/machine/bert_input.jpg)

输入的第一个字符为[CLS]，在这里字符[CLS]表达的意思很简单 - Classification （分类）。

模型输出

![](/public/upload/machine/bert_output.jpg)

每个位置返回的输出都是一个隐藏层大小的向量（基本版本BERT为768）。以文本分类为例，我们重点关注第一个位置上的输出（第一个位置是分类标识[CLS]） bert 希望它最后的输出代表整个序列的信息。该向量现在可以用作我们选择的分类器的输入，在论文中指出使用单层神经网络作为分类器就可以取得很好的效果。例子中只有垃圾邮件和非垃圾邮件，如果你有更多的label，你只需要增加输出神经元的个数即可，另外把最后的激活函数换成softmax即可。

![](/public/upload/machine/bert_classify.jpg)

## 训练方式

[一文读懂深度学习：从神经元到BERT](https://mp.weixin.qq.com/s/wrqxuMidw7HvgTVUvTBGng)相比语言模型任务只做预测下一个位置的单词，想要训练包含更多信息的语言模型，就需要让语言模型完成更复杂的任务，BERT 主要完成完形填空和句对预测的任务，即两个 loss：一个是 Masked Language Model，另一个是 Next Sentence Prediction。PS： **gpt可以视为将训练目标改为用最后一个输出向量预测下一个单词的bert？**

### Masked Language Model

MLM是为了训练深度双向语言表示向量，BERT 用了一个非常直接的方式，遮住句子里某些单词，让编码器预测这个单词是什么。
![](/public/upload/machine/bert_masked.jpg)

BERT 具体训练方法为：随机遮住 15%的单词作为训练样本。
1. 其中 80%用 masked token 来代替。
2. 10%用随机的一个词来替换。
3. 10%保持这个词不变。

直观上来说，只有 15%的词被遮盖的原因是性能开销，双向编码器比单向编码器训练要慢；选 80% mask，20%具体单词的原因是在 pretrain 的时候做了 mask，在特定任务微调如分类任务的时候，并不对输入序列做 mask，会产生 gap，任务不一致；10%用随机的一个词来替换，10%保持这个词不变的原因是让编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个 token 的表示向量，做了一个折中。

![](/public/upload/machine/bert_masked_token_prediction.jpg)

PS：训练时，自己知道自己mask 了哪个词，所以也是无监督了。

###  Next Sentence Prediction

预训练一个二分类的模型，来学习句子之间的关系。预测下一个句子的方法对学习句子之间关系很有帮助。

训练方法：正样本和负样本比例是 1：1，50%的句子是正样本，即给定句子 A 和 B，B 是 A 的实际语境下一句；负样本：在语料库中随机选择的句子作为 B。通过两个特定的 token[CLS]和[SEP]来串接两个句子，该任务在[CLS]位置输出预测。

![](/public/upload/machine/bert_next_sentence_prediction.jpg)

## Fine-tune

BERT的论文为我们介绍了几种BERT可以处理的NLP任务：
1. 短文本相似
    ![](/public/upload/machine/bert_similarity.jpg)
2. 文本分类
3. QA机器人
4. 语义标注
5. 特征提取 ==> rag 里的emebedding。 比如说我们自己训练一个类似BERT的模型，通过周围的词来预测完形填空试卷，“____是法国的首都”，通过一个模型训练词的上下文联系性之后，形成特定的词向量表。

针对不同任务，BERT 采用不同部分的输出做预测，分类任务利用[CLS]位置的 embedding，NER 任务利用每个 token 的输出 embedding。PS：最后一层的输出 选用[cls] 对应的embedding  或多个emebedding 套个FFNN + softmax，二分类或多分类任务就都可以解决了。**可以认为bert 对输入的文本做了一个编码**。

## 其它

WordPiece 分词会切词根， 切词根的目的是，很多词根是复用的，这样能减少此表大小（以3w 左右的词典，不然英文单词不只3w）