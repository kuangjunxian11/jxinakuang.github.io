---

layout: post
title: bert
category: 架构
tags: MachineLearning
keywords:  gcn

---

## 简介（未完成）

* TOC
{:toc}

BERT 是一个用 Transformers 作为特征抽取器的深度双向预训练语言理解模型。通过海量语料预训练，得到序列当前最全面的局部和全局特征表示。

[论文](https://arxiv.org/abs/1810.04805v1) 

bert 名称来自 Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional(相对gpt的单向来说，是双向的) representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. PS：ELMo 是基于rnn，在应用到下游任务时，还需要做一些模型结构的变动。有了一个训练好的bert之后，只需要再加一个额外的层，就可以适配各种任务。


## 模型结构

BERT的基础集成单元是Transformer的Encoder，BERT与Transformer 的编码方式一样。将固定长度的字符串作为输入，数据由下而上传递计算，每一层都用到了self attention，并通过前馈神经网络传递其结果，将其交给下一个编码器。

![](/public/upload/machine/bert_model.jpg)

模型输入

![](/public/upload/machine/bert_input.jpg)

输入的第一个字符为[CLS]，在这里字符[CLS]表达的意思很简单 - Classification （分类）。

模型输出

![](/public/upload/machine/bert_output.jpg)

每个位置返回的输出都是一个隐藏层大小的向量（基本版本BERT为768）。以文本分类为例，我们重点关注第一个位置上的输出（第一个位置是分类标识[CLS]） bert 希望它最后的输出代表整个序列的信息。该向量现在可以用作我们选择的分类器的输入，在论文中指出使用单层神经网络作为分类器就可以取得很好的效果。例子中只有垃圾邮件和非垃圾邮件，如果你有更多的label，你只需要增加输出神经元的个数即可，另外把最后的激活函数换成softmax即可。

![](/public/upload/machine/bert_classify.jpg)

## 训练方式

![](/public/upload/machine/bert_masked.jpg)

PS：训练时，自己知道自己mask 了哪个词，所以也是无监督了。

## 应用

BERT的论文为我们介绍了几种BERT可以处理的NLP任务：
1. 短文本相似
    ![](/public/upload/machine/bert_similarity.jpg)
2. 文本分类
3. QA机器人
4. 语义标注
5. 特征提取 ==> rag 里的emebedding

PS：最后一层的输出 选用[cls] 对应的embedding  或多个emebedding 套个FFNN + softmax，二分类或多分类任务就都可以解决了。

## 其它

WordPiece 分词会切词根， 切词根的目的是，很多词根是复用的，这样能减少此表大小（以3w 左右的词典，不然英文单词不只3w）