---

layout: post
title: 学习强化学习
category: 架构
tags: MachineLearning
keywords:  gcn

---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

## 简介（未完成）

* TOC
{:toc}

[没有强化学习基础也能看懂的PPO & GRPO](https://mp.weixin.qq.com/s/OIiNOMcuXERGVOghwNZ5Uw) 建议细读。
1. 只有Reward时的朴素做法：为什么会有问题？奖励信号波动很大，PS：激励一直为正
2. 引入 Critic，引入价值函数当参考线（baseline），从“只用 Reward” 进化成“用 Advantage 来衡量进步”
3. 加入 Clip 与 min 操作：防止更新过度
4. Reference Model：防止作弊、极端策略，新的行为不能和这个初始策略差太远，否则就要受到 KL惩罚。Actor 不会为了短期 Reward 而脱离原本合理的策略范畴，保证策略在演化过程中不至于“作弊”或偏得太离谱。
5. Critic（价值函数）通常需要跟 Actor 同等大小的网络去估计，否则很难评估到位，成本很高，而且有些场景（比如只在回答末尾才有一个整体 Reward）并不太适合训练出精细的价值函数。用同一问题的多条输出做平均，得到一个“相对评分”，再做标准化后作为 Advantage。


## 从训练机器人行走开始

[策略梯度法入门---强化学习](https://zhuanlan.zhihu.com/p/648788972)该例是要设计一个两腿机器人，使其能自动的行走。机器人左右腿的跨、膝、踝共有6个关节，都装有小电机，希望能自动控制它的6个小电机，使机器人能和人一样正常的行走。

强化学习需要一个软件系统，其基本组成包括：
1. 代理（Agent智能体）：是个软件，相当于机器人的大脑，是强化学习的核心。它可以接受环境状态的信息，还可以将计算的结果传输给环境。其中，负责计算的是个函数，称作策略（policy），是强化学习最重要的部分。
2. 环境（Environment）：代理以外的部分都是环境。和通常的环境概念不同，机器人所处的周边当然是环境，不同的是，机器人的躯体四肢都在代理之外，也都归于环境，比如躯干的高度，各个腿及各个关节的位置、速度等。此例中，环境状态使用31个观测值。包括

    1. 躯干沿Y和Z轴方向的坐标值，
    2. 躯干沿X、Y和Z方向的速度，
    3. 躯干旋转的角度和角速度，
    4. 6个关节的角度和角速度，
    5. 脚和地面之间的接触力。

3. 状态（State）：指环境的状态。机器人一直在移动，所以，周围环境的状态，以及自身各关节的状态也在不断的变化。
4. 行动（Action）：指代理根据当前状态采取的动作，比如机器人向左或向右，向前或向后移动等。
5. 奖励（Reward）：代理在当前状态下，采取了某个行动之后，会获得环境的反馈，称作奖励。但可能是奖励，也可能是惩罚，实际是代理对行动的评价。在强化学习中，奖励非常重要，因为样本没有标签，所以奖励起到引领学习的作用。

使机器人正常行走要做的工作。让两腿机器人正常行走，要做的工作是，用正确的指令控制每个关节，使机器人的腿和躯干正确的移动，这需要有六个关节的扭矩指令。在给定的环境状态下，如何得到正确的指令，这就是强化学习要做的工作。用传统方法开发机器人的行走程序，要人工设计逻辑、循环、控制器及参数等等，需要很多的环路，非常复杂。而强化学习的思想极为简单，它不考虑整个过程的具体步骤，不进行一步步的具体设计，而是把这一切复杂工作都塞到一个函数里，这个函数称作策略函数。策略收到环境传来的31个状态值，会自动计算出6个可供执行的指令,指令正确，机器人就会正常的行走。可以看出，强化学习中，机器人能正常行走的关键，就是这个策略函数。

所以下面的重点是：智能体里的策略函数是什么形式？它如何进行学习？如何通过环境状态值计算出6个正确的指令，使机器人能正常的行走。
1. 策略是个函数，因其过于复杂，很难用显性的公式来表式。对于这种连续且复杂的问题，强化学习是采用功能强大的神经网络来近似这个函数。这里策略神经网络是用的多层感知机（最基本的前馈神经网络），并以此为例进行说明。
    ![](/public/upload/machine/rl_robot.jpg)
    神经网络包含多个隐藏层，每层都有数百个神经元，没有足够多的神经元，网络无法拟合这么复杂的非线性函数，不可能将31个观察值正确的映射到6个动作。但是，神经元过多，将花费更多的时间训练，还容易得到过拟合的逻辑。所以，如何选择网络结构，包括网络层的数量，各层如何连接，以及每层神经元的数量等等，需要丰富的经验和知识找到最佳平衡点，使训练即可行又有效。
2. 神经网络的学习过程。策略函数的学习过程，可以选择仿真或真实行走的方式。方法是让机器人不断的行走，不断的调整策略函数里的参数w和b，直至得到能使机器人正常行走的网络模型（策略函数）。一般前馈网络的学习，样本都有标签（标准答案），在一次前向传播后，计算出结果，先求计算结果与标签的误差（损失函数），再求损失函数对各个节点的偏导数（梯度），然后求出损失函数对各个参数w和b的偏导数，用这些参数的偏导数来调整各个参数。但是，强化学习是以状态观测值作为输入（样本），**没有标签，当然也就没有损失函数，那么求谁的梯度（偏导数）？没有梯度怎么修改参数w和b？**强化学习的作法是设计一个奖励函数，用动作的奖励函数作准则，来调整网络的参数。强化学习里奖励函数的作用，相当于一般神经网络里的损失函数。做法是，每输入31个环境状态值，用策略函数计算出6个动作指令，然后执行这6个动作指令，当然这会改变了环境的状态值，包括躯干和6个关节的位置和速度等，然后再计算这个动作的奖励函数值，通过提高奖励函数值来调整网络的各个参数w和b。注意，不是像一般前馈网络学习那样，通过减小损失函数来调整参数w和b。
3. 奖励函数。奖励函数是人工精心设计的，其作用相当于前馈网络或循环网络的标签，是引导机器人正常行走的根据。此例已经设计好的奖励函数是
    $$
    r_t = v_x -3y^2 - 50z^2 + 25xx - 0.22xx
    $$
    其中$v_x$是前进速度，$v_x$越大，机器人走得越快。y是侧向位移，使机器人沿着一条直线移动，不向左右偏移，越小越好。z是机器人重心的垂直位移，是要机器人的躯干保持一定的高度，不摔倒、不跳跃或蹲着走路，越小越好。其余设置，这里不逐一深究。总之，这个奖励函数值越大，机器人走的就越好。奖励函数非常重要，但其设置是个难点，需要丰富的经验和专业知识。
4. 策略函数的学习过程。现在，策略函数是一个神经网络（多层感知机），策略能否做出正确的动作指令，取决于网络里的所有参数w和b。而评价一个动作优劣的标准，是这个动作的奖励函数，奖励函数的值越高，该动作就越好。所以，强化学习是求出奖励函数对各个参数w和b的梯度，让参数w和b沿着奖励函数的梯度方向调整，这样奖励函数值就会升高，下一次遇到同样的环境状态，策略计算出的指令就会更好。就这样，反复的调整参数w和b，最终得到一个可以时时做出正确指令的神经网络。
    ![](/public/upload/machine/rl_policy_func.jpg)
    具体作法大意是
    1. 输入31个环境值；
    2. 网络计算得到6个指令，执行指令改变了环境，检 测得到了31个新环境值；
    3. 用检测到的31个新环境值，求奖励函数对各个参数w和b的梯度（偏导数），即反向传播；
    4. 修改网络所有的参数w和b；
    5. 返回到1，进行下一循环学习。
    就这样，代理不断与环境交互，不断修改策略的参数，最终，在任何状态情况下，它都会采取最有利的行动。强化学习的过程，就是不断优化这个策略的过程，整个过程，是计算机自己在学习正确的参数，实际是个反复优化完善网络的过程。上述2执行6个指令后得到的新环境状态值，是环境检测出来的，和策略网络没有连在一起，用它求奖励函数值没问题，直接代入就行。但用这个新环境状态值对各个参数w和b求偏导数，就有问题了，因为它和策略网络根本没连接上。


PS： rl和dl都是在想办法，算一个loss，优化w和b。

## 从Policy Gradient到PPO到GRPO

[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://zhuanlan.zhihu.com/p/20530204146)

一些基础概念（LLM训练任务下好理解版）：
- $\pi$（Policy，策略）：即LLM模型
- $\theta$（Parameter，参数）：即模型参数
- $\tau$（Trajectory，轨迹）：即输出序列，此处可以理解为输出的一整个句子，每一个输出token即为action。
- s（State，交互状态）：即上文，初始状态即为$s_1$
- a（Action，交互行为）：即输出的token，可以简单理解为每个字符。（实际上一个字不等于一个token）

那么，我们可以得到模型参数$\theta$ 下生成序列$\tau$ 的概率如下：

$$
p_\theta(\tau) = p(s_1) p_\theta(a_1|s_1) p(s_2|s_1, a_1) \ldots = p(s_1) \prod_{t=1}^{T} p_\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)
$$

Reward Function（奖励函数定义，即输出序列$\tau$ 能获得的奖励）：
$$
R(\tau) = \sum_{t=1}^{T} r_t
$$

因此可得，模型参数 \( \theta \) 下的Expected Reward（期望奖励）：
$$
\overline{R}_\theta = \sum_{\tau} R(\tau) p_\theta(\tau)
$$

综上，我们希望调整模型参数 $ \theta $ 使这个期望奖励越大越好，因此可得Policy Gradient公式如下，期望做gradient ascent最大化期望奖励：

$$
\nabla \overline{R}_\theta = \sum_{\tau} R(\tau) \nabla p_\theta(\tau)
$$
$$
= \sum_{\tau} R(\tau) p_\theta(\tau) \nabla \log p_\theta(\tau) \quad \text{\# Note: } \nabla f(x) = f(x) \nabla \log f(x)
$$
$$
= E_{\tau \sim p_\theta(\tau)} [R(\tau) \nabla \log p_\theta(\tau)]
$$
$$
\approx \frac{1}{N} \sum_{n=1}^{N} R(\tau^n) \nabla \log p_\theta(\tau^n) \quad \text{\# 实际上就是N个轨迹近似期望，使期望reward最大化}
$$
$$
= \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^n) \nabla \log p_\theta(a_t^n | s_t^n) \quad \text{\# 环境无法作用gradient 所以可以移除}
$$

直观理解：在某个state（上文）下执行某个action（token）使得最后整个输出$ \tau$ 的reward是正的时候，我们应该**增加这个输出的几率**，反之减少。PS：rl就是，判断哪个输出更好，把这个输出的概率提高多少，还是算loss并反向传播。 

但是，如果仔细看上述公式，会发现 $ R(\tau) $ 即reward恒为正的情况，那会导致一直在增加任何token的输出概率。但我们实际操作中是用sample的方式来训练，这就导致某些项实际上因为没被sample到而导致输出概率下降（实际ground truth是要提升）。所以我们希望引入一个baseline（b）让reward不是恒为正。公式变成如下：
$$
\nabla \overline{R}_\theta = \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} (R(\tau^n) - b) \nabla \log p_\theta(a_t^n | s_t^n)
$$

通常我们可以将baseline设置为reward的期望值，即 $ b \approx E[R(\tau)] $。我们知道最终输出的是一个序列 \( \tau \)，且在算reward时是以 $ \tau $ 的粒度计算的。即使整体的reward是正的，也不意味着序列中的每一个action都是有收益的（如：说了一串废话，最后才说对结果）。因此，更合理的做法是我们需要给每一个action合适的credit。

首先，我们会有一些假设（注意：并不一定什么情况下都适用，应根据具体情况使用不同的reward function）：

1. reward应单独为每个action计算（前面的）
$$ 
R(\tau^n) \rightarrow \sum_{t'=t}^{T_n} r_{t'}^n \quad \text{\# 计算当前action后所有reward的总和作为当前action的reward} 
$$

2. 越快完成任务应越重要，距离越远贡献越小
$$  
R(\tau^n) \rightarrow \sum_{t'=t}^{T_n} r_{t'}^n \rightarrow \sum_{t'=t}^{T_n} \gamma^{t'-t} r_{t'}^n \quad \text{\# } \gamma \text{为时间衰减函数} 
$$ 

实际上 $  R(\tau^n) - b $  这一项其实是在算在某个state下执行某个action比执行其他action有多好，也就是我们常说的Advantage Function，可以表示为 $  A^\theta(s_t, a_t) $ ，因此综上公式可以写作：

$$ 
\nabla \overline{R}_\theta \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} A^\theta(s_t, a_t) \nabla \log p_\theta(a_t^n | s_t^n)
$$ 
$$ 
= E_{(s_t, a_t) \sim \pi_\theta} [A^\theta(s_t, a_t) \nabla \log p_\theta(a_t^n | s_t^n)]
$$ 

### PPO(Proximal Policy Optimization)

由于Policy Gradient是一个on policy的方式，在每一轮更新后都需要重新收集训练数据，这无疑是很麻烦的。因此我们希望将其改造为一个off policy的方式，能重复使用一批数据去更新参数。PS：on policy同策略，就是用自己策略生成的数据去训练自己，对应的概念叫离策略（off-policy），就是用其他策略生成的数据来训练自己这个策略。

Importance Sampling: $  E_{x \sim p} [f(x)] = E_{x \sim q} [f(x) \frac{p(x)}{q(x)}] $  【解释起来太复杂此处不过多赘述】

基于上述公式Policy Gradient可以写作：
$$ 
\nabla \overline{R}_\theta \approx E_{(s_t, a_t) \sim \pi_\theta} [A^\theta(s_t, a_t) \nabla \log p_\theta(a_t^n | s_t^n)]
$$ 
$$ 
= E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \frac{P_\theta(s_t, a_t)}{P_{\theta'}(s_t, a_t)} A^\theta(s_t, a_t) \nabla \log p_\theta(a_t^n | s_t^n) \right]
$$ 
$$ 
= E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \frac{P_\theta(s_t | a_t)}{P_{\theta'}(s_t | a_t)} \frac{P_\theta(s_t)}{P_{\theta'}(s_t)} A^\theta(s_t, a_t) \nabla \log p_\theta(a_t^n | s_t^n) \right]
$$ 
$$ 
= E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \frac{P_\theta(s_t | a_t)}{P_{\theta'}(s_t | a_t)} A^\theta(s_t, a_t) \nabla \log p_\theta(a_t^n | s_t^n) \right] \quad \text{\# 假设 } s_t \text{ 不受到 } \theta \text{ 影响}
$$ 

因此我们能得到一个新的Objective Function：
$$ 
J^{\theta^k} (\theta) = E_{(s_t, a_t) \sim \pi_{\theta^k}} \left[ \frac{P_\theta(s_t | a_t)}{P_{\theta^k}(s_t | a_t)} A^{\theta^k} (s_t, a_t) \right] \quad \text{\# Note: } \nabla f(x) = f(x) \nabla \log f(x)
$$ 
实际上是 $ \theta' $ 在交互，然后更新 $ \theta $。

为了保证训练后比之前不至于差距太大（基础能力等不会变化太大），引入一个类正则项如下：
$$ 
J^{PPO}_\theta (\theta) = J^{\theta'} (\theta) - \beta KL(\theta, \theta')
$$ 
Note：这个KL散度说的不是参数分布距离，而是输出分布距离（behavior distance）。实际应用中可以使用adaptive KL penalty用于动态调整 $ \beta $
$$ 
\text{if } KL(\theta, \theta^k) > KL_{\max}, \text{ increase } \beta
$$ 
$$ 
\text{if } KL(\theta, \theta^k) < KL_{\min}, \text{ decrease } \beta
$$ 

但更多的大家会用以下PPO2公式，简化计算（KL太难算）：
$$ 
J^{PPO2}_\theta (\theta) \approx \sum_{(s_t, a_t)} \min \left( \frac{p_\theta(a_t | s_t)}{p_{\theta^k}(a_t | s_t)} A^{\theta^k} (s_t, a_t), \text{clip} \left( \frac{p_\theta(a_t | s_t)}{p_{\theta^k}(a_t | s_t)}, 1 - \varepsilon, 1 + \varepsilon \right) A^{\theta^k} (s_t, a_t) \right)
$$ 
本质上就是替换了adaptive KL penalty这一块，保证两个分布间不要差距太大。
Note: clip(a,b,c); 当`a<b`时，取b；当`a>c`时，取c；其余取a。

### Group Relative Policy Optimization(群体相对策略优化)

![](/public/upload/machine/ppo_grpo.jpg)

GRPO舍弃了传统PPO算法中的Critic模型(通常与策略模型大小相同)部分，转而通过直接从群体得分中估算baseline。

具体来说，对于每一个问题 $  q $ ，GRPO会从旧的策略模型参数 $  \pi_{\theta_{\text{old}}} $  中采样一组输出 $ \{o_1, o_2, o_3, \ldots, o_G\} $，然后通过最大化GRPO目标函数以优化当前策略模型参数 $ \pi_\theta $。

辅助理解：不同的策略模型 $\pi$ 实际上是一个模型在不同参数阶段下的版本。

具体可以按如下理解，
- $ \pi_{\theta_{\text{old}}} $：上一轮模型参数的模型，可以理解为 $ \pi_\theta $ 上一个iteration的模型。
- $ \pi_\theta $：最新的模型参数的模型（正在更新的）。
- $ \pi_{\theta_{\text{ref}}} $：初始模型参数。

原文公式如下图所示：

$$ 
\mathcal{J}_{\text{GRPO}}(\theta) = E \left[ q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(O \mid q) \right]
$$ 

$$ 
\frac{1}{G} \sum_{i=1}^G \left( \min \left( \frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)} A_i, \text{clip} \left( \frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}, 1-\varepsilon, 1+\varepsilon \right) A_i \right) - \beta D_{KL} \left( \pi_\theta \mid \mid \pi_{\text{ref}} \right) \right),
$$ 

$$ 
D_{KL} \left( \pi_\theta \mid \mid \pi_{\text{ref}} \right) = \frac{\pi_{\text{ref}}(o_i \mid q)}{\pi_\theta(o_i \mid q)} - \log \frac{\pi_{\text{ref}}(o_i \mid q)}{\pi_\theta(o_i \mid q)} - 1,
$$ 

$ \varepsilon $ 控制学习步长上限，保证每一轮学习不至于与上一轮偏移过大。主要是防止策略崩溃。

$ \beta $ 控制原始能力偏移惩罚的程度。主要是缓解灾难性遗忘的问题。


[DeepSeek的GRPO算法是什么？ - 梦想成真的回答 - 知乎](
https://www.zhihu.com/question/10766825126/answer/113322446718)最核心的就是$\frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}A_i$
1. q 表示此次的问题
2. $o_i$ 表示旧的policy model 的第i个输出策略
3. $\pi_\theta(o_i \mid q)$ 表示给定问题q，policy model 输出$o_i$的概率
4. $\pi_{\theta_{\text{old}}}(o_i \mid q)$ 表示给定问题q，旧的上一轮梯度下降后的 policy model 输出$o_i$的概率

进而，我们知道了$\frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}$其实就是policy 模型学习过程中的偏移程度，很明显，loss中要最大化$\frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}$，然这不能无上限的 maximize，因为PPO其实是希望模型一点一点学，每一次不能偏移过多，因此clip项就是在做梯度剪裁，KL散度也在进行正则。$\frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}$越大，比如说大于1，那么说明新的策略模型更倾向于输出$o_i$这个策略。

我们再说优势函数 (Advantage Function) $A_i$，R1中非常简单直接，就是reward score 做个标准化，得到的$A_i$就是第i个决策在多个决策中相比较baseline 好多少或者坏多少。如果$A_i$>0，那么就是正向激励，否则，$A_i$< 0 就是负向激励。

通过如下公式计算出：

$$ 
A_i = \frac{r_i - \text{mean} \left( \{r_1, r_2, \cdots, r_G\} \right)}{\text{std} \left( \{r_1, r_2, \cdots, r_G\} \right)}.
$$ 

我们现在可以讲解 $\frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}A_i$ 为啥要把这两项乘到一起？其实原因很简单，$\frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}$就是第i个输出的策略倾向，$A_i$可以理解为一种激励。如果新的策略模型$\pi_\theta(o_i \mid q)$比旧的策略模型$\pi_{\theta_{\text{old}}}(o_i \mid q)$更加希望输出$o_i$ 的策略，并且优势函数$A_i$> 0，也就是获得的reward score好于平均值，那么当然值得鼓励了，所以最大化这一项没有问题。如果$A_i$< 0，说明使用这种策略反倒效果不好，此时$\frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}A_i$是一个负数，最大化负数不就是减少$\frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}$，也就是减少输出 $o_i$的策略的概率吗？对于一堆候选策略，当你做的比别人好，我鼓励你，当你做的比别人差，我尽量减少这种情况再次出现，模型就一步一步朝着更好的优化方向走了

![](/public/upload/machine/grpo.jpg)

接下来说clip，$\text{clip} \left( \frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}, 1-\varepsilon, 1+\varepsilon \right)A_i$，clip( ) 这一项表示把$\frac{\pi_\theta(o_i \mid q)}{\pi_{\theta_{\text{old}}}(o_i \mid q)}$ 限制在 $1-\varepsilon$ 到$1+\varepsilon$之间，类似梯度剪裁。$\varepsilon$是一个超参数。很简单，GRPO并不希望新的策略模型更新的太快，本身强化学习就不好训练，容易训飞。

GRPO希望 $ 
D_{KL} \left( \pi_\theta \mid \mid \pi_{\text{ref}} \right) = \frac{\pi_{\text{ref}}(o_i \mid q)}{\pi_\theta(o_i \mid q)} - \log \frac{\pi_{\text{ref}}(o_i \mid q)}{\pi_\theta(o_i \mid q)} - 1$ 尽量变小一点。需要注意的是，这并不是标准的KL散度，让$D_{KL} \left( \pi_\theta \mid \mid \pi_{\text{ref}} \right)$变小就是让$\frac{\pi_{\text{ref}}(o_i \mid q)}{\pi_\theta(o_i \mid q)}$变小。$\pi_{\text{ref}}(o_i \mid q)$是上一个epoch训练好的$\pi_\theta(o_i \mid q)$，新的epoch中保持不变，而$\pi_\theta(o_i \mid q)$是新的epoch正在训练的策略模型。这意味着这个kl变体损失项是为了让分子和分母基本上相同，当x=1的时候（参考$x-lnx-1$ 的曲线），kl变体损失项最小为0。

我们稍微总结下GRPO：GRPO去掉了value model，仅仅只训练policy model，并且使用reward score标准化的方式作为baseline，让模型找到更优的解法，为了更稳定的训练，clip项和KL散度变种都是辅佐让模型不要训飞，一步步慢慢学的手段。

## 其它

[从Policy Gradient到REINFORCE++，万字长文梳理强化学习最新进展](https://mp.weixin.qq.com/s/mGlObqTANspHGkujzCmY5A)

[浅谈 RL 里面的 KL 散度](https://zhuanlan.zhihu.com/p/26370587517)个人认为，RL与SFT的区别在于，SFT是token level的0/1奖励，RL是句子level的离散奖励。当然，RL也可以往过程奖励（PRM）或者规则奖励（rule-based）去走。往过程奖励走，无非是引入一个sub-sentence level的监督信号，介于整句（或者说答案）与单个词之间的监督信息。往规则走，是希望整体系统不要被Reward Model所束缚，如果数据质量足够高+基座足够优秀，那么就不需要花里胡哨的reward形式，直接使用rule-based reward就行。这里的reward大多是（-1、0.2、1）这种三段式设计，本质上和SFT的0/1是差不多的。如果我们对（-1, 0.2, 1）做一次softmax，那么就变成了（0.08, 0.26, 0.64）。从某个视角来说，也算是one-hot label的平滑形式。大家都喜欢说，RL是泛化的，SFT是记忆的。我觉得，之所以造成这种现象，是因为RL学的比较难，所以聚焦于方法论，而SFT学的比较简单，那么就容易掉入过拟合陷阱，也就是SFT记住但无泛化。正是因为RL学的比较难，那么RL的acc涨的是比较慢的，好处但是就是，真的往解题的技巧上学。


