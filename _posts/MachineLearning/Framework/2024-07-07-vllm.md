---

layout: post
title: 大模型推理服务框架vLLM
category: 架构
tags: MachineLearning
keywords: llm vLLM

---

* TOC
{:toc}

## 简介（未完成）

[vLLM代码及逻辑介绍](https://zhuanlan.zhihu.com/p/675322419) 值得细读。

[如何让vLLM适配一个新模型](https://zhuanlan.zhihu.com/p/680636375)

## 从PagedAttention开始

[图解大模型计算加速系列之：vLLM核心技术PagedAttention原理](https://mp.weixin.qq.com/s/-5EniAmFf1v9RdxI5-CwiQ)用KV cache做推理时的一些特点：
1. 随着prompt数量变多和序列变长，KV cache也变大，对gpu显存造成压力
2. 由于输出的序列长度无法预先知道，所以我们很难提前为KV cache量身定制存储空间
为KV cache分配存储空间的常规方式：当我们的服务接收到一条请求时，它会为这条请求中的prompts分配gpu显存空间，其中就包括对KV cache的分配。由于推理所生成的序列长度大小是无法事先预知的，所以大部分框架会按照(batch_size, max_seq_len)这样的固定尺寸，在gpu显存上预先为一条请求开辟一块连续的矩形存储空间。然而，这样的分配方法很容易引起“gpu显存利用不足”的问题，进而影响模型推理时的吞吐量。你会发现它们的毛病在于太过“静态化”。当你无法预知序列大小时，你为什么一定要死板地为每个序列预留KV cache空间呢？为什么不能做得更动态化一些，即“用多少占多少”呢？这样我们就能减少上述这些存储碎片，使得每一时刻推理服务能处理的请求更多，提高吞吐量。你可能会有以下疑问：
1. vLLM是通过什么技术，动态地为请求分配KV cache显存，提升显存利用率的？
    1.  在最原始的做法中，不使用虚拟内存，，程序直接对物理内存进行操作，决定使用它的哪些存储地址。如果你只跑一个进程，那还好说。但如果需要运行多个进程时，麻烦就来了：由于我直接操作了物理内存地址，所以我在为自己的进程分配物理内存时，还要考虑别的进程是如何分配物理内存的（别人已经占用的我不能用）。这样不同进程间的耦合性太高了，给开发带来难度。有没有一种办法，让各个进程间的开发能够相互独立呢？一种直觉的做法是：给每个进程分配一个虚拟内存。每个进程在开发和运行时，可以假设这个虚拟内存上只有自己在跑，**虚拟内存负责统一规划代码、数据等如何在物理内存上最终落盘**。==> 虚拟内存的分页管理
    2. 对于相同数据对应的KV cache，能复用则尽量复用（不同request 的逻辑块指向相同的物理块）；无法复用时，再考虑开辟新的物理空间（触发物理块copy-on-write机制）。
2. 当采用动态分配显存的办法时，虽然明面上同一时刻能处理更多的prompt了，但因为没有为每个prompt预留充足的显存空间，如果在某一时刻整个显存被打满了，而此时所有的prompt都没做完推理，那该怎么办？
    1. 当有一堆请求来到vLLM服务器上时，vLLM需要一个调度原则来安排如何执行这些请求，这个调度原则概括如下：先来的请求先被服务（First-Come-First-Serve, FCFS）；如有抢占的需要，后来的请求先被抢占（preemption）。当一堆请求来到vLLM服务器做推理，导致gpu显存不足时，vLLM暂停这堆请求中最后到达的那些请求的推理，同时将它们相关的KV cache从gpu上释放掉，以便为更早到达的请求留出足够的gpu空间，让它们完成推理任务。如果不这样做的话，各个请求间相互争夺gpu资源，最终将导致没有任何一个请求能完成推理任务。等到先来的请求做完了推理，vLLM调度器认为gpu上有足够的空间了，就能恢复那些被中断的请求的执行了。在资源不足的情况下，暂时中断一些任务的执行，这样的举动就被称为“抢占（preemption）”。

## 请求调度
[vLLM（二）架构概览](https://zhuanlan.zhihu.com/p/681716326)
[图解大模型计算加速系列：vLLM源码解析1，整体架构](https://mp.weixin.qq.com/s/r_t6_zMvPT7za82MZX4oRA) Offline Batched Inference
```python
from vllm import LLM, SamplingParams
prompts = ["Hello, my name is",
           "The president of the United States is",
           "The capital of France is",
           "The future of AI is",]
# 采样参数
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
# 初始化vLLM offline batched inference实例，并加载指定模型
llm = LLM(model="facebook/opt-125m")
# 推理
outputs = llm.generate(prompts, sampling_params)
# 对每一条prompt，打印其推理结果
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```
在传统离线批处理中，我们每次给模型发送推理请求时，都要：等一个batch的数据齐全后，一起发送；整个batch的数据一起做推理；等一个batch的数据全部推理完毕后，一起返回推理结果。这种“团体间等成员到齐，再一起行动”的行为，就被称为“同步”。但推理内核引擎（LLMEngine）在实际运作时，batch_size是可以动态变更的：在每一个推理阶段（prefill算1个推理阶段，每个decode各算1个推理阶段）处理的batch size可以根据当下显存的实际使用情况而变动（vLLM会用自己的调度策略从waiting队列中依次取数，加入running队列中，直到它认为取出的这些数据将会打满它为1个推理阶段分配好的显存。此时waiting队列中可能还会剩一些数据）。但将LLMEngine包装成离线批处理形式后，所有的数据必须等到一起做完推理才能返给我们。所以从体感上，我们可能很难感知到内核引擎的“动态”逻辑。

也正是因为LLMEngine这种“动态处理”的特性，才使得它同时也能成为异步在线服务的内核引擎：当一条条请求发来时，它们都先进入LLMEngine调度器（Scheduler）的waiting队列中（实际并不是直接进入waiting队列中的，而是在传给LLMEngine前先进入asyncio.Queue()中，然后再由LLMEngine调度进waiting队列中的，这些细节我们也放在后面说，这里不影响理解就行）。此时模型正常执行它的1个推理阶段，调度器也正常处理新来的请求。当模型准备执行下1个推理阶段时，调度器再根据设定的策略，决定哪些数据可以进入running队列进行推理。由于在线服务是异步的，先推理完成的数据就可以先发给客户端了（如果采用流式传输，也可以生成多少先发多少）。vLLM在实现在线服务时，采用uvicorn部署fastapi app实例，以此实现异步的请求处理。而核心处理逻辑封装在AsyncLLMEngine类中（它继承自LLMEngine）。所以，只要我们搞懂了LLMEngine，对vLLM的这两种调用方式就能举一反三了。

vLLM对请求的调度处理流程：
1. 当一堆请求来到vLLM服务器上时，按照First-Come-First-Serve（FCFS）原则，优先处理那些最早到来的请求。
2. 当gpu资源不足时，为了让先来的请求能尽快做完推理，vLLM会对那些后到来的请求执行“抢占”，即暂时终止它们的执行。
3. 一旦vLLM决定执行抢占操作，它会暂停处理新到来的请求。在此期间，它会将被抢占的请求相关的KV block全部交换（swap）至cpu上。等交换完成后，vLLM才会继续处理新到来的请求。
4. 当vLLM认为gpu有足够资源时，它会将cpu上的KV block重新加载回gpu，恢复被抢占请求的执行（recomputation）

![](/public/upload/machine/vllm_arch.jpg)

vLLM的两种调用方式与内核引擎LLMEngine的关系如下。

![](/public/upload/machine/vllm_ui_and_scheduler.jpg)

对于多gpu，vLLM是怎么处理的呢？
1. 首先，vLLM有一个中央调度器（Scheduler），它负责计算和管理每张卡上KV cache从逻辑块到物理块的映射表(block tables)
2. 在做分布式计算时，Schedular会将映射表广播到各张卡上，每张卡上的Cache engine接收到相关信息后，负责管理各卡上的KV block。
上图中给出的例子，是用张量模型并行（megatron-lm）做分布式推理时的情况，所以图中每个worker上写的是model shard。在张量并行中，各卡上的输入数据相同，只是各卡负责计算不同head的KV cache。所以这种情况下，各卡上的逻辑块-物理块的映射关系其实是相同的（用的同一张block table），只是各卡上物理块中实际存储的数据不同而已。

## 源码

```
vllm
    /attention
    /core
        /block
        /block_manager_v1.py
        /evictor_v1.py
        /scheduler.py
    /distributed
    /engine
        /openai
        /api_server.py
        /llm.py
    /entrypoints
    /executor  # 模型执行
```

不难发现LLMEngine是vLLM的核心逻辑。有几个函数
1. add_request()：该方法将每一个请求包装成vLLM能处理的数据类型SequenceGroup，并将其加入调度器（Scheduler）的waiting队列中。
2. abort_request：在推理过程中，并不是所有的请求都能有返回结果。比如客户端断开连接时，这个请求的推理就可以终止了（abort），这个函数就被用来做这个操作。

### 入口

[图解大模型计算加速系列：vLLM源码解析2，调度器策略(Scheduler)](https://mp.weixin.qq.com/s/UCdqQUM_9a36uXkO36wpSg)


```python
class LLM:
    def __init__(self,model,tokenizer,quantization,...) -> None:
        engine_args = EngineArgs(...)
        self.llm_engine = LLMEngine.from_engine_args(engine_args, usage_context=UsageContext.LLM_CLASS)
        ...
    def generate(self,prompts,sampling_params,...)-> List[RequestOutput]:
        # 将输入数据传给LLMEngine
        self._validate_and_add_requests(  inputs=inputs,params=sampling_params,lora_request=lora_request,)
        # 执行推理
        outputs = self._run_engine(use_tqdm=use_tqdm)
        return LLMEngine.validate_outputs(outputs, RequestOutput)
    def _add_request(self,inputs,params,lora_request,...)    ) -> None:
        # 在vLLM内核运算逻辑中，1个prompt算1个request，需要有1个全局唯一的request_id
        request_id = str(next(self.request_counter))
        self.llm_engine.add_request(request_id,inputs,params,lora_request=lora_request)                  
    def _run_engine(self,...):
        outputs: List[Union[RequestOutput, EmbeddingRequestOutput]] = []
        while self.llm_engine.has_unfinished_requests():
            step_outputs = self.llm_engine.step()
            for output in step_outputs:
                if output.finished:
                     outputs.append(output)
        return sorted(outputs, key=lambda x: int(x.request_id))
```

所以，想要知道调度器的运作流程，我们只要从LLMEngine的add_request()和step()两个函数入手就好了。

```python
class LLMEngine:
    def __init__(  self,model_config,cache_config,parallel_config,scheduler_config,device_config,...) -> None:
        self.model_executor = executor_class(...)
        ...
    def _add_processed_request(self,request_id,processed_inputs,...)
        block_size = self.cache_config.block_size
        seq_id = next(self.seq_counter)
        eos_token_id = self._get_eos_token_id(lora_request)
        seq = Sequence(seq_id, processed_inputs, block_size, eos_token_id, lora_request)
        # 把每1个prompt包装成一个SequenceGroup对象
        seq_group = self._create_sequence_group_with_xx(...)
        min_cost_scheduler = self.scheduler[costs.index(min(costs))]
        # 把包装成SequenceGroup对象的数据加入调度器（Scheduler）的waiting队列，等待处理。
        min_cost_scheduler.add_seq_group(seq_group)
```
### add_reques/SequenceGroup

为什么要把每个prompt都包装成一个SequenceGroup实例？SequenceGroup又长什么样呢？
1. 在一般的推理场景中，我们通常给模型传1个prompt及相关的采样参数，让模型来做推理。此时你的输入可能长下面这样：`("To be or not to be,",SamplingParams(temperature=0.8, top_k=5, presence_penalty=0.2))`。
2. 但在其余的场景中，模型decoding的策略可能更加复杂，例如： 
    1. Parallel Sampling：你传给模型1个prompt，希望模型基于这个prompt，给出n种不同的output `("What is the meaning of life?",
SamplingParams(n=2, temperature=0.8, top_p=0.95, frequency_penalty=0.1))`
    2. Beam Search：你传给模型1个prompt，在采用Beam Search时，每个推理阶段你都会产出top k个output，其中k被称为Beam width（束宽） `("It is only with the heart that one can see rightly",
SamplingParams(n=3, best_of=3, use_beam_search=True, temperature=0.0)),`
总结来说，可能出现"1个prompt -> 多个outputs"的情况。那是否能设计一种办法，对1个prompt下所有的outputs进行集中管理，来方便vLLM更好做推理呢？一个seq_group中的所有seq共享1个prompt。"1个prompt -> 多个outputs"这样的结构组成一个SequenceGroup实例，其中每组"prompt -> output"组成一个序列（seq，属于Sequence实例），每个seq下有若干状态(status)属性。我们来通过一个具体的例子，更好感受一下SequenceGroup的作用：

![](/public/upload/machine/vllm_sequence_group.jpg)
1. 在推理开始之前，这个seq_group下只有1条seq，它就是prompt，状态为waiting。
2. 在第1个推理阶段，调度器选中了这个seq_group，由于它的采样参数中n = 4，所以在做完prefill之后，它会生成4个seq，它们的状态都是running。
3. 在若干个推理阶段后，gpu上的资源不够了，这个seq_group不幸被调度器抢占（preemption），它相关的KV block也被swap out到cpu上。此时所有seq的状态变为swapped。这里要注意，当一个seq_group被抢占时，对它的处理有两种方式：
    1. Swap：如果该seq_group下的seq数量 > 1，此时会采取swap策略，即把seq_group下【所有】seq的KV block从gpu上卸载到cpu上。（seq数量比较多，直接把算出的KV block抛弃，比较可惜）
    2. Recomputation：如果该seq_group下的seq数量 = 1，此时会采取recomputation策略，即把该seq_group相关的物理块都释放掉，然后将它重新放回waiting队列中。等下次它被选中推理时，就是从prefill阶段开始重新推理了，因此被称为“重计算”。（seq数量少，重新计算KV block的成本不高）
4. 又过了若干个推理阶段，gpu上的资源又充足了，此时执行swap in操作，将卸载到cpu上的KV block重新读到gpu上，继续对该seq_group做推理，此时seq的状态又变为running。
5. 又过了若干个推理阶段，该seq_group中有1个seq已经推理完成了，它的状态就被标记为finish，此后这条已经完成的seq将不参与调度。
6. 又过了若干个推理阶段，这个seq_group下所有的seq都已经完成推理了，这样就可以把它作为最终output返回了。

```python
class SequenceGroup:
    def __init__(self,request_id,seqs,...) -> None:
        self.request_id = request_id
        self.seqs_dict = {seq.seq_id: seq for seq in seqs} # 其中每个seq是一个Sequence对象。
        self.sampling_params = sampling_params # 采样参数
    # 该seq_group在剩余生命周期内并行running的最大seq数量。
    def get_max_num_running_seqs(self) -> int: 
        ...
class Sequence:
    """Stores the data, status, and block information of a sequence.    """
    def __init__(self,seq_id,inputs,block_size,eos_token_id,...)-> None:
        ...
        # 1个Sequence实例下维护着属于自己的逻辑块列表
        self.logical_token_blocks: List[LogicalTokenBlock] = []
        # Initialize the logical token blocks with the prompt token ids.
        self._append_tokens_to_blocks(self.prompt_token_ids)
        ...
    def _append_tokens_to_blocks(self, token_ids: List[int]) -> None:
        cursor = 0
        while cursor < len(token_ids):
            if not self.logical_token_blocks:
                self._append_logical_block()

            last_block = self.logical_token_blocks[-1]
            if last_block.is_full():
                self._append_logical_block()
                last_block = self.logical_token_blocks[-1]

            num_empty_slots = last_block.get_num_empty_slots()
            last_block.append_tokens(token_ids[cursor:cursor +num_empty_slots])                                 
            cursor += num_empty_slots
    def _append_logical_block(self) -> None:
        block = LogicalTokenBlock(block_number=len(self.logical_token_blocks),block_size=self.block_size,)
        self.logical_token_blocks.append(block)
    # 计算一个逻辑块的hash值，是对文本内容的hash
    def hash_of_block(self, logical_idx: int) -> int:
```
当一个seq只有prompt时，这个方法负责给prompt分配逻辑块；当这个seq开始产出output时，这个方法负责给每一个新生成的token分配逻辑块

### step()：调度器策略

在1个推理阶段中，调度器是通过什么策略来决定要送哪些seq_group去做推理的？

```python
class LLMEngine:
    def step(self) -> ...:
        seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()
        if not scheduler_outputs.is_empty():
            execute_model_req = ExecuteModelRequest(seq_group_metadata_list,blocks_to_swap_in,blocks_to_swap_out,...)
            output = self.model_executor.execute_model(execute_model_req=execute_model_req)
        else:
            output = []
        request_outputs = self._process_model_outputs(output,scheduler_outputs.scheduled_seq_groups,...)
        ...
        return request_outputs            
class Scheduler:
    def __init__( self,scheduler_config,cache_config,lora_config,...)-> None:
        ...
        BlockSpaceManagerImpl = BlockSpaceManager.get_block_space_manager_class(version)
        # 物理块管理器，有v1、v2等版本
        self.block_manager = BlockSpaceManagerImpl(block_size=...,num_gpu_blocks,num_cpu_blocks,...)
        self.waiting: Deque[SequenceGroup] = deque() # 存放所有还未开始做推理的seq_group
        self.running: Deque[SequenceGroup] = deque() # 存放当前正在做推理的seq_group
        self.swapped: Deque[SequenceGroup] = deque() # 存放被抢占的seq_group
    def add_seq_group(self, seq_group: SequenceGroup) -> None:
        # Add sequence groups to the waiting queue.
        self.waiting.append(seq_group)
```
1. waiting队列中的数据都没有做过prefill，每个seq_group下只有1个seq（prompt）
2. running队列中存放着上一个推理阶段被送去做推理的所有seq_group。
2. swapped队列中存放着之前调度阶段中被抢占的seq_group
running队列中的seq_group不一定能继续在本次调度中被选中做推理，这是因为gpu上KV cache的使用情况一直在变动，以及waiting队列中持续有新的请求进来的原因。所以调度策略的职责就是要根据这些变动，对送入模型做推理的数据做动态规划。

在1次推理中，所有seq_group要么一起做prefill（来自running队列），要么一起做decode（来自running + swapped队列）。先做哪个呢？
1. 以swapped是否非空为判断入口。如果当前调度步骤中swapped队列非空，说明在之前的调度步骤中这些可怜的seq_group因为资源不足被抢占，而停滞了推理。所以根据FCFS规则，当gpu上有充足资源时，我们应该先考虑它们，而不是考虑waiting队列中新来的那些seq_group。
2. waiting队列是否达到调度间隔阈值。在做推理时，waiting队列中是源源不断有seq_group进来的，一旦vLLM选择调度waiting队列，它就会停下对running/swapped中seq_group的decode处理，转而去做waiting中seq_group的prefill，也即vLLM必须在新来的seq_group和已经在做推理的seq_group间取得一种均衡：既不能完全不管新来的请求，也不能耽误正在做推理的请求。所以“waiting队列调度间隔阈值”就是来控制这种均衡的。
    1. 调度间隔设置得太小，每次调度都只关心waiting中的新请求，这样发送旧请求的用户就迟迟得不到反馈结果。且此时waiting队列中积累的新请求数量可能比较少，不利于做batching，浪费了并发处理的能力。
    2. 调度间隔设置得太大，waiting中的请求持续挤压，同样对vLLM推理的整体吞吐有影响。
3. 内存空间是否充足？
    1. 如果决定调度 waiting队列的seq_group，从waiting中取出一个seq_group，必须先判断：gpu上是否有充足的空间为该seq_group分配物理块做prefill（给每个seq分配若干个token的位置），判断的入口代码为`can_allocate = self.block_manager.can_allocate(seq_group)`
    2. 从running队列中调度seq_group时，我们也会判断是否能为该seq_group分配物理块。这时，我们的物理块空间是用来做decode的（给每个seq分配1个token的位置）。对于1个seq_group，除了那些标记为“finish”的seq外，其余seqs要么一起送去推理，要么一起不送去推理。即它们是集体行动的。判断能否对一个正在running的seq_group继续做推理的最保守的方式，就是判断当前可用的物理块数量是否至少为n。

```python
class Scheduler:
    def _schedule(self) -> SchedulerOutputs:
        # 如果swapped队列为空
        if not self.swapped:
            scheduled: List[SequenceGroup] = [] # 记录本次被调度的seq_group
            # 计算Scheduler running队列中还没有执行完的seq数量
            num_curr_seqs = sum(seq_group.get_max_num_running_seqs() for seq_group in self.running)
            leftover_waiting_sequences = deque()
            # 当waiting队列中有等待处理的请求，且当前时刻可以处理请求
            while self._passed_delay(now) and self.waiting:
                seq_group = self.waiting[0] # 取出waiting队列中的第一个请求，也即最早到达的请求（seq_group）
                waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)
                num_prefill_tokens = waiting_seqs[0].get_len()
                # 决定是否能给当前seq_group分配物理块
                can_allocate = self.block_manager.can_allocate(seq_group)
                # 若是延迟分配，则说明现在没有足够的block空间，所以跳出while循环（不继续对waiting队列中的数据做处理了）
                if can_allocate == AllocStatus.LATER:
                    break
                elif can_allocate == AllocStatus.NEVER:
                    for seq in waiting_seqs:
                        seq.status = SequenceStatus.FINISHED_IGNORED
                    ignored_seq_groups.append(seq_group) # 记录因为太长而无法处理的seq_group
                    self.waiting.popleft() # 将该seq_group从waiting队列中移除
                    continue
                #  走到这一步时，说明当前seq_group已经通过上述种种验证，可以被加入本次调度中执行了先将其从waiting队列中移出
                self.waiting.popleft()
                self._allocate(seq_group) # 为当前seq_group分配物理块
                self.running.append(seq_group)  # 将当前seq_group放入running队列中
            if scheduled or ignored_seq_groups:
                # 如果本次有被调度的seq_group（scheduled非空） 或者本次有被设置为不再处理的seq_group(ignored_seq_groups非空) 就将其包装成SchedulerOutputs对象
                ...
                return scheduler_outputs
        # 如果swap队列非空，且本次没有新的需要被发起推理的seq_group,
        self.running = self.policy.sort_by_priority(now, self.running)
        running: Deque[SequenceGroup] = deque()
        preempted: List[SequenceGroup] = []
        while self.running:
            seq_group = self.running.popleft() # 取出running队列中最早到来的seq_group
            # 对于running队列中这个最早到来的seq_group，检查对于其中的每一个seq，是否能至少分配一个物理块给它，如果不能的话
            while not self.block_manager.can_append_slot(seq_group):
                if self.running:       # 如果从running队列中取出最早达到的seq_group后，running队列还是非空
                    victim_seq_group = self.running.pop()  # 抢占running队列中最晚到来的seq_group（可怜的被害者）
                    self._preempt(victim_seq_group, blocks_to_swap_out)
                    preempted.append(victim_seq_group)
                else:  # 那就只能抢占这个最早到达的seq_group了
                    self._preempt(seq_group, blocks_to_swap_out)
                    preempted.append(seq_group)
                    break
        ...                           
```

### 物理块管理器

[图解大模型计算加速系列：vLLM源码解析3，块管理器（BlockManager）上篇](https://mp.weixin.qq.com/s/JBAqo2780Sf_ov86am3MjQ)
[图解大模型计算加速系列：vLLM源码解析3，Prefix Caching](https://mp.weixin.qq.com/s/bAY4OGqQlEeBaITIwxQEuw) 未读

```python
class BlockSpaceManagerV1(BlockSpaceManager):
    """Manages the mapping between logical and physical token blocks."""
    def __init__(self,block_size: int,num_gpu_blocks: int,num_cpu_blocks: int,...)-> None:
        self.gpu_allocator = ...
        self.cpu_allocator = ...
        # 负责维护每个seq下的物理块列表，本质上它是一个字典，形式如{seq_id: List[PhysicalTokenBlock]}
        self.block_tables: Dict[int, BlockTable] = {} 
    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:
        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]
        num_required_blocks = len(seq.logical_token_blocks)
        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()
        # 如果设备中所有的物理块数量 - 该seq实际需要的物理块数量 < 水位线block数量，则不分配（说明当前seq太长了）
        if (self.num_total_gpu_blocks - num_required_blocks < self.watermark_blocks):
            return AllocStatus.NEVER
        # 如果设备中可用的物理块数量 - 该seq实际需要的block数量 >= 水位线block数量，则分配
        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:
            return AllocStatus.OK
        # 否则，现在不能分配，但可以延迟分配
        else:
            return AllocStatus.LATER
    # 是否至少能为这个seq_group下的每个seq都分配1个空闲物理块
    def can_append_slots(self,seq_group: SequenceGroup,num_lookahead_slots: int = 0) -> bool:               
        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()
        num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)
        return num_seqs <= num_free_gpu_blocks
    #  为当前seq_group分配物理块做prefill
    def allocate(self, seq_group: SequenceGroup) -> None:
        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]
        num_prompt_blocks = len(seq.logical_token_blocks) # 计算该seq的逻辑块数量
        block_table: BlockTable = []
        for logical_idx in range(num_prompt_blocks):
            block = self.gpu_allocator.allocate()
            block.ref_count = seq_group.num_seqs()
            block_table.append(block)
        # prefill阶段，这个seq_group下所有的seq共享一个prompt，也即共享这个prompt代表的物理块
        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):
            self.block_tables[seq.seq_id] = block_table.copy()
    def append_slots(self,seq: Sequence,) -> Optional[Tuple[int, int]]:
        ...
        if ...
            maybe_new_block = self._maybe_promote_last_block( seq, last_block)
            block_table[-1] = maybe_new_block
            return ...
        if ...
            # 如果当前物理块的slots满了
            new_block = self._allocate_last_physical_block(seq)
            block_table[-1] = new_block
        return ...
                   
```

BlockAllocator又分成两种类型：
1. CachedBlockAllocator：按照prefix caching的思想来分配和管理物理块。在原理篇中，我们提过又些prompts中可能含有类似system message（例如，“假设你是一个能提供帮助的行车导航”）等prefix信息，带有这些相同prefix信息的prompt完全可以共享用于存放prefix的物理块，这样既节省显存，也不用再对prefix做推理。
2. UncachedBlockAllocator：正常分配和管理物理块，没有额外实现prefix caching的功能。

物理块结构（一切尽在注释中）：
```python
# vllm/block.py
class PhysicalTokenBlock:
    """Represents the state of a block in the KV cache."""
    def __init__(self,device: Device,block_number: int, block_size: int,block_hash: int,num_hashed_tokens: int,) -> None:
        self.device = device # 设备，gpu/cpu
        self.block_number = block_number # 该物理块在对应设备上的全局block index
        self.block_size = block_size # 该物理块的尺寸（即槽位数量，默认为16）
        self.block_hash = block_hash  # 该物理块的hash值 （在prefix caching场景下使用，非此场景则附值为-1）
        self.num_hashed_tokens = num_hashed_tokens  # 该物理块的hash值是由多少个前置token计算而来的（prefix caching场景下使用，非此场景则附值为0）
        self.ref_count = 0  # 该物理块被多少个逻辑块引用
        self.last_accessed = DEFAULT_LAST_ACCESSED_TIME # 该物理块最后一次被使用的时间（prefix caching场景下使用，非此场景则附值为-1）
        self.computed = False   # 该物理块是否被计算过（prefix caching场景下使用）
```
逻辑块结构（一切尽在注释中），它是Sequence实例（seq）下维护的一个属性。
```python
# vllm/block.py
class LogicalTokenBlock:
    """A block that stores a contiguous chunk of tokens from left to right.Logical blocks are used to represent the states of the corresponding physical blocks in the KV cache.
    KV cache的逻辑块
    """

    def __init__(self,block_number: int, # 逻辑块的序号
            block_size: int, # 每个逻辑块中有多少个槽位（默认为16）
        ) -> None:
        self.block_number = block_number
        self.block_size = block_size

        # 逻辑块刚初始化时，将其中的每个token_id都初始化为_BLANK_TOKEN_ID（-1）
        self.token_ids = [_BLANK_TOKEN_ID] * block_size 
        # 当前逻辑块中已经装下的token的数量
        self.num_tokens = 0
    def is_full(self) -> bool:
        """判断当前逻辑块是否已经被装满"""
        return self.num_tokens == self.block_size

    def append_tokens(self, token_ids: List[int]) -> None:
        """将给定的一些token_ids装入当前逻辑块中"""
        # 给定的token_ids的长度必须 <= 当前逻辑块剩余的槽位
        assert len(token_ids) <= self.get_num_empty_slots()
        # 当前逻辑块第一个空槽的序号
        curr_idx = self.num_tokens
        # 将这些tokens装进去
        self.token_ids[curr_idx:curr_idx + len(token_ids)] = token_ids
        # 更新当前逻辑块中tokens的数量
        self.num_tokens += len(token_ids)
    def get_token_ids(self) -> List[int]:
        """获取当前逻辑块中所有被装满的位置的token_ids"""
        return self.token_ids[:self.num_tokens]
    def get_last_token_id(self) -> int:
        """获取当前逻辑块所所有被装满的位置的最后一个token_id"""
        assert self.num_tokens > 0
        return self.token_ids[self.num_tokens - 1]
```
每个seq维护自己的一份逻辑块列表，BlockManager中的self.block_tables（形式如：`{seq_id: List[PhysicalBlock]}`）则记录者每个seq下的物理块列表，通过seq这个中介，我们维护起“逻辑块->物理块”的映射。

```python
class UncachedBlockAllocator(BlockAllocatorBase):
    def __init__(self, device: Device,block_size: int,num_blocks: int,) -> None:
        self.device = device # 设备：cpu/gpu
        self.block_size = block_size # 该设备上每个物理块的槽位数，默认为16
        self.num_blocks = num_blocks # 该设备上留给KV cache的总物理块数量
         self.free_blocks: BlockTable = []
        for i in range(num_blocks):
            # vllm/vllm/block.py
            # 定义物理块
            block = PhysicalTokenBlock(device=device,block_number=i,block_size=block_size,block_hash=-1,num_hashed_tokens=0)        
            self.free_blocks.append(block)
    def allocate(self,block_hash,num_hashed_tokens: int = 0) -> PhysicalTokenBlock:
        block = self.free_blocks.pop()
        block.ref_count = 1 # 该物理块首次有逻辑块引用了，所以ref_count=1
        return block

class CachedBlockAllocator(BlockAllocatorBase):
    def __init__(self, device: Device,block_size,num_blocks,eviction_policy,...)-> None:
        ...
        self.evictor: Evictor = make_evictor(eviction_policy) # 默认是LRUEvictor            
    #  为哈希值为block_hash分配一个物理块
    def allocate_block(self, block_hash: int,num_hashed_tokens: int) -> PhysicalTokenBlock:
        ...
```
当一个物理块没有任何逻辑块引用时（例如一个seq刚做完整个推理），这时它理应被释放。但是在prefix caching的前提下，我们的优化思想是：即使这个物理块当前没有用武之地，可是如果不久之后来了一个新seq，它的prefix（例如system message）和这个物理块指向的内容高度一致，那么这个物理块就可以被重复使用，以此减少存储和计算开销。所以，我们设置一个驱逐器（evictor）类，它的free_tables属性将用于存放这些暂时不用的物理块。此时，该设备上全部可用的物理块 = 正在被使用/等待被使用的物理块数量 + evictor的free_tables中的物理块数量。在prefill阶段，当我们想创建一个物理块时，我们先算出这个物理块的hash值，然后去free_tables中看有没有可以重复利用的物理块，有则直接复用。如果没有可以重复利用的hash块，那这时我们先检查下这台设备剩余的空间是否够我们创建一个新物理块。如果可以，就创建新物理块。如果此时没有足够的空间创建新物理块，那么我们只好从free_tables中驱除掉一个物理块，为这个新的物理块腾出空间，驱逐策略如下：
1. 先根据LRU（Least Recently Used）原则，驱逐较老的那个物理块
2. 如果找到多个最后一次使用时间相同的老物理块，那么则根据max_num_tokens原则，驱逐其hash值计算中涵盖tokens最多的那个物理块。
3. 如果这些老物理块的LRU和max_num_tokens还是一致的话，那就从它们中随机驱逐一个

使用prefix caching，是不是就意味着两个seq的prompt必须完全一致，才可以重复利用物理块呢？
1. prefill阶段。比如下图，seq1的block0～2都可以复用seq0的，但是`hash(seq1 block3) != hash(seq0 block3)`，因此我们需要为seq1 block3（红色）开辟新空间。因为KV cache的计算也需要考虑位置编码的原因,hash值的计算考虑了当前block及其之前所有block所维护的token值，也是为了找到最长可复用的prefix。
    ![](/public/upload/machine/vllm_prefix_caching_prefill.jpg)     
2. 在decode阶段，当两个seq物理块还没满的时候，我们会给它附一个相互不重复的默认hash值，两个seq继续做decode（风平浪静的美丽日子），当一个seq用完当前物理块的所有slots时，我们再对这个物理块重新做hash计算（对所有的prefix计算一次hash值），拿着这个new_hash，我们去cached_blocks（当前正在被使用的物理块列表）和free_tables（驱逐器的冷宫，曾经被使用的物理块列表）寻找。如果找到可以复用的物理块，我们就释放当前这个物理块，复用旧物理块，如果没有找到可以复用的物理块，我们就把当前这个物理块的旧hash值从cached_blocks中释放掉，取而代之以新hash值。
    ![](public/upload/machine/vllm_prefix_caching_decode.jpg)

### cacheManager

### 模型执行


vllm需要重写模型，因为要适配paged attention等相关优化

[vllm模型执行笔记: LLMEngine, Executor, Worker, ModelRunner - 陈star的文章 - 知乎](https://zhuanlan.zhihu.com/p/706685260)

## 其他 

[vllm代码更新太频繁，我该怎么办？](https://mp.weixin.qq.com/s/kd7uFFFEciFbuuX8FHJfyQ)

