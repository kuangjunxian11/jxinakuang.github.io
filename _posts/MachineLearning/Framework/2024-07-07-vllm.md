---

layout: post
title: 大模型推理服务框架vLLM
category: 架构
tags: MachineLearning
keywords: llm vLLM

---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


* TOC
{:toc}

## 简介（未完成）

[vLLM代码及逻辑介绍](https://zhuanlan.zhihu.com/p/675322419) 值得细读。

[如何让vLLM适配一个新模型](https://zhuanlan.zhihu.com/p/680636375)

[vLLM（二）架构概览](https://zhuanlan.zhihu.com/p/681716326)

[图解大模型计算加速系列：vLLM源码解析1，整体架构](https://mp.weixin.qq.com/s/r_t6_zMvPT7za82MZX4oRA) 未读

[图解大模型计算加速系列：vLLM源码解析2，调度器策略(Scheduler)](https://mp.weixin.qq.com/s/UCdqQUM_9a36uXkO36wpSg) 未读

## PagedAttention

[图解大模型计算加速系列之：vLLM核心技术PagedAttention原理](https://mp.weixin.qq.com/s/-5EniAmFf1v9RdxI5-CwiQ)用KV cache做推理时的一些特点：
1. 随着prompt数量变多和序列变长，KV cache也变大，对gpu显存造成压力
2. 由于输出的序列长度无法预先知道，所以我们很难提前为KV cache量身定制存储空间
为KV cache分配存储空间的常规方式：当我们的服务接收到一条请求时，它会为这条请求中的prompts分配gpu显存空间，其中就包括对KV cache的分配。由于推理所生成的序列长度大小是无法事先预知的，所以大部分框架会按照(batch_size, max_seq_len)这样的固定尺寸，在gpu显存上预先为一条请求开辟一块连续的矩形存储空间。然而，这样的分配方法很容易引起“gpu显存利用不足”的问题，进而影响模型推理时的吞吐量。你会发现它们的毛病在于太过“静态化”。当你无法预知序列大小时，你为什么一定要死板地为每个序列预留KV cache空间呢？为什么不能做得更动态化一些，即“用多少占多少”呢？这样我们就能减少上述这些存储碎片，使得每一时刻推理服务能处理的请求更多，提高吞吐量。你可能会有以下疑问：
1. vLLM是通过什么技术，动态地为请求分配KV cache显存，提升显存利用率的？
    1.  在最原始的做法中，不使用虚拟内存，，程序直接对物理内存进行操作，决定使用它的哪些存储地址。如果你只跑一个进程，那还好说。但如果需要运行多个进程时，麻烦就来了：由于我直接操作了物理内存地址，所以我在为自己的进程分配物理内存时，还要考虑别的进程是如何分配物理内存的（别人已经占用的我不能用）。这样不同进程间的耦合性太高了，给开发带来难度。有没有一种办法，让各个进程间的开发能够相互独立呢？一种直觉的做法是：给每个进程分配一个虚拟内存。每个进程在开发和运行时，可以假设这个虚拟内存上只有自己在跑，**虚拟内存负责统一规划代码、数据等如何在物理内存上最终落盘**。==> 虚拟内存的分页管理
    2. 对于相同数据对应的KV cache，能复用则尽量复用（不同request 的逻辑块指向相同的物理块）；无法复用时，再考虑开辟新的物理空间（触发物理块copy-on-write机制）。
2. 当采用动态分配显存的办法时，虽然明面上同一时刻能处理更多的prompt了，但因为没有为每个prompt预留充足的显存空间，如果在某一时刻整个显存被打满了，而此时所有的prompt都没做完推理，那该怎么办？
    1. 当有一堆请求来到vLLM服务器上时，vLLM需要一个调度原则来安排如何执行这些请求，这个调度原则概括如下：先来的请求先被服务（First-Come-First-Serve, FCFS）；如有抢占的需要，后来的请求先被抢占（preemption）。当一堆请求来到vLLM服务器做推理，导致gpu显存不足时，vLLM暂停这堆请求中最后到达的那些请求的推理，同时将它们相关的KV cache从gpu上释放掉，以便为更早到达的请求留出足够的gpu空间，让它们完成推理任务。如果不这样做的话，各个请求间相互争夺gpu资源，最终将导致没有任何一个请求能完成推理任务。等到先来的请求做完了推理，vLLM调度器认为gpu上有足够的空间了，就能恢复那些被中断的请求的执行了。在资源不足的情况下，暂时中断一些任务的执行，这样的举动就被称为“抢占（preemption）”。

## 请求调度

vLLM对请求的调度处理流程：
1. 当一堆请求来到vLLM服务器上时，按照First-Come-First-Serve（FCFS）原则，优先处理那些最早到来的请求。
2. 当gpu资源不足时，为了让先来的请求能尽快做完推理，vLLM会对那些后到来的请求执行“抢占”，即暂时终止它们的执行。
3. 一旦vLLM决定执行抢占操作，它会暂停处理新到来的请求。在此期间，它会将被抢占的请求相关的KV block全部交换（swap）至cpu上。等交换完成后，vLLM才会继续处理新到来的请求。
4. 当vLLM认为gpu有足够资源时，它会将cpu上的KV block重新加载回gpu，恢复被抢占请求的执行（recomputation）

![](/public/upload/machine/vllm_arch.jpg)

对于多gpu，vLLM是怎么处理的呢？
1. 首先，vLLM有一个中央调度器（Scheduler），它负责计算和管理每张卡上KV cache从逻辑块到物理块的映射表(block tables)
2. 在做分布式计算时，Schedular会将映射表广播到各张卡上，每张卡上的Cache engine接收到相关信息后，负责管理各卡上的KV block。
上图中给出的例子，是用张量模型并行（megatron-lm）做分布式推理时的情况，所以图中每个worker上写的是model shard。在张量并行中，各卡上的输入数据相同，只是各卡负责计算不同head的KV cache。所以这种情况下，各卡上的逻辑块-物理块的映射关系其实是相同的（用的同一张block table），只是各卡上物理块中实际存储的数据不同而已。